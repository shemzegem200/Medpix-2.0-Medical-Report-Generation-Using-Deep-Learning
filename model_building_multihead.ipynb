{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ad9c51-ff2c-4eec-8a15-fce9dae20d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.1+cu126\n",
      "torchvision: 0.22.1+cu126\n",
      "âœ… torchvision fully loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "print(\"âœ… torchvision fully loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf99192-6b64-436d-a50d-3ed9f5038408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b6c44f-c0c7-4e24-b881-5418ab7e4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_REPORT_LEN = 120\n",
    "EMBED_DIM = 256\n",
    "TEXT_DIM = 768\n",
    "KG_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "# NUM_WORKERS = 4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa24167f-d5e8-4528-8a88-4aaa0fbe550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facda5f1-6fee-499a-99ab-fb18423a2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fc8218-9260-411b-a11d-91c86cc8fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_TOKENS = {\n",
    "    \"Head\": {\n",
    "        \"bos\": \"<HEAD_BOS>\",\n",
    "        \"eos\": \"<HEAD_EOS>\"\n",
    "    },\n",
    "    \"Thorax\": {\n",
    "        \"bos\": \"<THORAX_BOS>\",\n",
    "        \"eos\": \"<THORAX_EOS>\"\n",
    "    },\n",
    "    \"Abdomen\": {\n",
    "        \"bos\": \"<ABDOMEN_BOS>\",\n",
    "        \"eos\": \"<ABDOMEN_EOS>\"\n",
    "    },\n",
    "    \"Spine and Muscles\": {\n",
    "        \"bos\": \"<SPINE_BOS>\",\n",
    "        \"eos\": \"<SPINE_EOS>\"\n",
    "    },\n",
    "    \"Reproductive and Urinary System\": {\n",
    "        \"bos\": \"<GU_BOS>\",\n",
    "        \"eos\": \"<GU_EOS>\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98215c67-208f-4dc4-84c4-31d3f4d4909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30533\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "special_tokens = {\n",
    "    \"pad_token\": \"<PAD>\",\n",
    "    \"additional_special_tokens\": []\n",
    "}\n",
    "\n",
    "for loc in LOCATION_TOKENS:\n",
    "    special_tokens[\"additional_special_tokens\"].append(\n",
    "        LOCATION_TOKENS[loc][\"bos\"]\n",
    "    )\n",
    "    special_tokens[\"additional_special_tokens\"].append(\n",
    "        LOCATION_TOKENS[loc][\"eos\"]\n",
    "    )\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394cbe35-d47e-48ff-a4b8-e3fb761c3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MedPixDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Normalize NaNs early (VERY IMPORTANT)\n",
    "        self.df = self.df.fillna(\"\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def load_image(self, img_path):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def parse_image_list(self, s):\n",
    "        # CSV stores lists as strings: \"['path1', 'path2']\"\n",
    "        if s == \"\" or s == \"[]\":\n",
    "            return []\n",
    "        return ast.literal_eval(s)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -------- Images --------\n",
    "        ct_images = []\n",
    "        mri_images = []\n",
    "\n",
    "        ct_paths = self.parse_image_list(row[\"CT_image_paths\"])\n",
    "        mri_paths = self.parse_image_list(row[\"MRI_image_paths\"])\n",
    "\n",
    "        for p in ct_paths:\n",
    "            ct_images.append(self.load_image(p))\n",
    "\n",
    "        for p in mri_paths:\n",
    "            mri_images.append(self.load_image(p))\n",
    "\n",
    "        # -------- Text Encoder Input --------\n",
    "        text_input = row[\"combined_text\"]\n",
    "\n",
    "        # -------- Target Report --------\n",
    "        report = row[\"findings\"]\n",
    "\n",
    "        # -------- Location (KG routing) --------\n",
    "        location_category = row[\"Location Category\"]\n",
    "\n",
    "        return {\n",
    "            \"uid\": row[\"U_id\"],\n",
    "            \"ct_images\": ct_images,     # list[Tensor]\n",
    "            \"mri_images\": mri_images,   # list[Tensor]\n",
    "            \"text_input\": text_input,   # str\n",
    "            \"report\": report,           # str\n",
    "            \"location\": location_category\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252ca940-92c8-4b9c-8494-74ab73f4afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch items contain:\n",
    "    - ct_images: list[Tensor]\n",
    "    - mri_images: list[Tensor]\n",
    "    - text_input: str\n",
    "    - report: str\n",
    "    - location: str\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- CT images ----------\n",
    "    max_ct = max(len(b[\"ct_images\"]) for b in batch)\n",
    "    ct_imgs, ct_masks = [], []\n",
    "\n",
    "    for b in batch:\n",
    "        imgs = b[\"ct_images\"]\n",
    "        if len(imgs) == 0:\n",
    "            dummy = torch.zeros(3, 224, 224)\n",
    "            imgs = [dummy]\n",
    "\n",
    "        pad = max_ct - len(imgs)\n",
    "        imgs = imgs + [torch.zeros_like(imgs[0])] * pad\n",
    "        mask = [1] * (len(imgs) - pad) + [0] * pad\n",
    "\n",
    "        ct_imgs.append(torch.stack(imgs))\n",
    "        ct_masks.append(torch.tensor(mask))\n",
    "\n",
    "    ct_imgs = torch.stack(ct_imgs)      # (B, N_ct, 3, H, W)\n",
    "    ct_masks = torch.stack(ct_masks)    # (B, N_ct)\n",
    "\n",
    "    # ---------- MRI images ----------\n",
    "    max_mri = max(len(b[\"mri_images\"]) for b in batch)\n",
    "    mri_imgs, mri_masks = [], []\n",
    "\n",
    "    for b in batch:\n",
    "        imgs = b[\"mri_images\"]\n",
    "        if len(imgs) == 0:\n",
    "            dummy = torch.zeros(3, 224, 224)\n",
    "            imgs = [dummy]\n",
    "\n",
    "        pad = max_mri - len(imgs)\n",
    "        imgs = imgs + [torch.zeros_like(imgs[0])] * pad\n",
    "        mask = [1] * (len(imgs) - pad) + [0] * pad\n",
    "\n",
    "        mri_imgs.append(torch.stack(imgs))\n",
    "        mri_masks.append(torch.tensor(mask))\n",
    "\n",
    "    mri_imgs = torch.stack(mri_imgs)    # (B, N_mri, 3, H, W)\n",
    "    mri_masks = torch.stack(mri_masks)  # (B, N_mri)\n",
    "\n",
    "    # ---------- Text encoder input ----------\n",
    "    text_inputs = [b[\"text_input\"] for b in batch]\n",
    "    text_enc = tokenizer(\n",
    "        text_inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------- Decoder target ----------\n",
    "    reports = [b[\"report\"] for b in batch]   # <-- RAW ground truth\n",
    "    report_enc = tokenizer(\n",
    "        reports,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_REPORT_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------- Location ----------\n",
    "    locations = [b[\"location\"] for b in batch]\n",
    "\n",
    "    return {\n",
    "        \"ct_images\": ct_imgs,\n",
    "        \"ct_masks\": ct_masks,\n",
    "        \"mri_images\": mri_imgs,\n",
    "        \"mri_masks\": mri_masks,\n",
    "        \"text_input_ids\": text_enc[\"input_ids\"],\n",
    "        \"text_attention_mask\": text_enc[\"attention_mask\"],\n",
    "        \"report_input_ids\": report_enc[\"input_ids\"],\n",
    "        \"report_attention_mask\": report_enc[\"attention_mask\"],\n",
    "        \"reports\": reports,              # <-- RAW ground-truth strings\n",
    "        \"locations\": locations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6311cd02-d41d-48b2-ab18-b72fe0451f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPX1009\n",
      "CT images: 2\n",
      "MRI images: 0\n",
      "Text length: 379\n",
      "Report length: 152\n",
      "Location: Reproductive and Urinary System\n"
     ]
    }
   ],
   "source": [
    "ds = MedPixDataset(r\"C:\\fyp_manish_shyam_phase2\\data\\df_overall.csv\", transform=image_transform)\n",
    "\n",
    "sample = ds[0]\n",
    "print(sample[\"uid\"])\n",
    "print(\"CT images:\", len(sample[\"ct_images\"]))\n",
    "print(\"MRI images:\", len(sample[\"mri_images\"]))\n",
    "print(\"Text length:\", len(sample[\"text_input\"]))\n",
    "print(\"Report length:\", len(sample[\"report\"]))\n",
    "print(\"Location:\", sample[\"location\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949bc98e-c4fe-4b56-ab60-b52f9a9017f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda71f90-3260-4a5e-b38b-1a9e4773b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT images shape: torch.Size([3, 2, 3, 224, 224])\n",
      "CT masks shape: torch.Size([3, 2])\n",
      "MRI images shape: torch.Size([3, 1, 3, 224, 224])\n",
      "MRI masks shape: torch.Size([3, 2])\n",
      "Text input ids shape: torch.Size([3, 90])\n",
      "Report input ids shape: torch.Size([3, 73])\n",
      "Raw reports count: 3\n",
      "First report preview:\n",
      " Bladder with thickened wall and diverticulum on the right. Diverticulum is mostly likely secondary to chronic outflow obstruction. Prostate enlargement.\n",
      "Locations: ['Reproductive and Urinary System', 'Thorax', 'Reproductive and Urinary System']\n"
     ]
    }
   ],
   "source": [
    "# Taking 2â€“3 samples manually for doing a small sanity check\n",
    "batch_samples = [ds[i] for i in range(3)]\n",
    "\n",
    "batch = collate_fn(batch_samples)\n",
    "print(\"CT images shape:\", batch[\"ct_images\"].shape)\n",
    "print(\"CT masks shape:\", batch[\"ct_masks\"].shape)\n",
    "\n",
    "print(\"MRI images shape:\", batch[\"mri_images\"].shape)\n",
    "print(\"MRI masks shape:\", batch[\"mri_masks\"].shape)\n",
    "\n",
    "print(\"Text input ids shape:\", batch[\"text_input_ids\"].shape)\n",
    "print(\"Report input ids shape:\", batch[\"report_input_ids\"].shape)\n",
    "\n",
    "print(\"Raw reports count:\", len(batch[\"reports\"]))\n",
    "print(\"First report preview:\\n\", batch[\"reports\"][0][:200])\n",
    "\n",
    "print(\"Locations:\", batch[\"locations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0001bd89-f0fe-4dd5-b337-c529ddfd423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 671\n",
      "Train samples: 637\n",
      "Test samples:  34\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# ---- Load full dataset ----\n",
    "full_dataset = MedPixDataset(\n",
    "    r\"C:\\fyp_manish_shyam_phase2\\data\\df_overall.csv\",\n",
    "    transform=image_transform\n",
    ")\n",
    "\n",
    "# ---- 80 / 20 split ----\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.95 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {dataset_size}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples:  {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b688522-5b96-4c03-b666-61dbafe7520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & Test loaders ready\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn\n",
    "    # , pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,          # No shuffle for test\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn\n",
    "    # , pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train & Test loaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "573ecc90-ab9e-40d2-b629-720770eeeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        for p in base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        base.fc = nn.Linear(base.fc.in_features, embed_dim)\n",
    "        self.cnn = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, 3, H, W)\n",
    "        return: (B, N, D)\n",
    "        \"\"\"\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "        feats = self.cnn(x)\n",
    "        feats = feats.view(B, N, -1)\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f6df810-7bae-41ac-92c1-78f1c5838f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_encoder = ImageEncoder().to(DEVICE)\n",
    "mri_encoder = ImageEncoder().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "444b5b38-d67b-4b11-9f60-49ab57ce0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_mean_pooling(feats, masks):\n",
    "    masks = masks.unsqueeze(-1).float()   # (B, N, 1)\n",
    "    summed = (feats * masks).sum(dim=1)\n",
    "    denom = masks.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ae7796-84ff-40a8-b22d-058f6ec9d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, embed_dim):\n",
    "        super().__init__()\n",
    "        self.lm = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        hidden_dim = self.lm.config.hidden_size\n",
    "        self.proj = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Mean pooling over tokens (causal models do NOT have CLS)\n",
    "        hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "\n",
    "        pooled = (hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        return self.proj(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6270bce5-2b14-4bc7-8433-d5ef8864ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30533, 768, padding_idx=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "text_encoder = TextEncoder(\n",
    "    model_name=TEXT_MODEL_NAME,\n",
    "    embed_dim=EMBED_DIM\n",
    ").to(DEVICE)\n",
    "\n",
    "text_encoder.lm.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f58656a4-447a-43af-9d8d-42dbf8ed7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, A_hat, X):\n",
    "        \"\"\"\n",
    "        A_hat: (N, N) normalized adjacency\n",
    "        X: (N, D)\n",
    "        \"\"\"\n",
    "        return F.relu(self.linear(A_hat @ X))\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(GCNLayer(dims[i], dims[i + 1]))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, A_hat, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(A_hat, X)\n",
    "        return X.mean(dim=0)   # graph-level embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1794a05c-21ea-42e7-9cbd-2384aebb02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A: (N, N) raw adjacency matrix\n",
    "    returns: (N, N) normalized adjacency with self-loops\n",
    "    \"\"\"\n",
    "    device = A.device\n",
    "    N = A.size(0)\n",
    "\n",
    "    # Add self-loops\n",
    "    A_tilde = A + torch.eye(N, device=device)\n",
    "\n",
    "    # Degree\n",
    "    D = A_tilde.sum(dim=1)\n",
    "\n",
    "    # D^{-1/2}\n",
    "    D_inv_sqrt = torch.pow(D, -0.5)\n",
    "    D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = torch.diag(D_inv_sqrt)\n",
    "\n",
    "    # Symmetric normalization\n",
    "    A_hat = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "    return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "555d53c6-1166-4d3c-8a54-3df126fc4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head: A_hat shape = torch.Size([4400, 4400])\n",
      "Thorax: A_hat shape = torch.Size([4400, 4400])\n",
      "Abdomen: A_hat shape = torch.Size([4400, 4400])\n",
      "Spine and Muscles: A_hat shape = torch.Size([4400, 4400])\n",
      "Reproductive and Urinary System: A_hat shape = torch.Size([4400, 4400])\n",
      "Shared X_nodes shape: torch.Size([4400, 4400])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "KG_LOCATION_MAP = {\n",
    "    \"Head\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Head_matrix.csv\",\n",
    "    \"Thorax\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Thorax_matrix.csv\",\n",
    "    \"Abdomen\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Abdomen_matrix.csv\",\n",
    "    \"Spine and Muscles\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Spine_and_Muscles_matrix.csv\",\n",
    "    \"Reproductive and Urinary System\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Reproductive_and_Urinary_System_matrix.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "A_hat_dict = {}\n",
    "\n",
    "# ---- Load and normalize adjacency matrices ----\n",
    "for loc, path in KG_LOCATION_MAP.items():\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    A = torch.tensor(\n",
    "        df.values,\n",
    "        dtype=torch.float32,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    A_hat = normalize_adjacency(A)\n",
    "    A_hat_dict[loc] = A_hat\n",
    "\n",
    "    print(f\"{loc}: A_hat shape = {A_hat.shape}\")\n",
    "\n",
    "# ---- Create shared X_nodes (identity) ----\n",
    "# Node count inferred from any adjacency matrix\n",
    "example_loc = next(iter(A_hat_dict))\n",
    "N_nodes = A_hat_dict[example_loc].shape[0]\n",
    "\n",
    "X_nodes = torch.eye(N_nodes, device=DEVICE)\n",
    "\n",
    "print(\"Shared X_nodes shape:\", X_nodes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "683603fc-db9c-4183-b56e-bfd3b0c5171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embed_dim * 4, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, ct_feat, mri_feat, text_feat, kg_feat):\n",
    "        \"\"\"\n",
    "        All inputs: (B, EMBED_DIM)\n",
    "        Output: (B, HIDDEN_DIM)\n",
    "        \"\"\"\n",
    "        fused = torch.cat(\n",
    "            [ct_feat, mri_feat, text_feat, kg_feat],\n",
    "            dim=-1\n",
    "        )\n",
    "        fused = self.dropout(fused)\n",
    "        return self.fc(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da79e065-95d6-4b57-bd4c-5ee9aa682688",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = FeatureFusion().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acbf9208-3be2-4d5d-93e0-6fabac1ecaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kg_embeddings(locations, gcn, X_nodes, A_hat_dict):\n",
    "    \"\"\"\n",
    "    locations: list[str], length B\n",
    "    returns: (B, KG_DIM)\n",
    "    \"\"\"\n",
    "\n",
    "    device = X_nodes.device\n",
    "\n",
    "    # 1. Compute KG embedding ONCE per unique location\n",
    "    unique_locations = set(locations)\n",
    "    location_to_embedding = {}\n",
    "\n",
    "    for loc in unique_locations:\n",
    "        A_hat = A_hat_dict[loc]              # (N, N)\n",
    "        kg_emb = gcn(A_hat, X_nodes)         # (KG_DIM,)\n",
    "        location_to_embedding[loc] = kg_emb\n",
    "\n",
    "    # 2. Assign embedding to each sample\n",
    "    kg_embeds = [\n",
    "        location_to_embedding[loc] for loc in locations\n",
    "    ]\n",
    "\n",
    "    return torch.stack(kg_embeds).to(device)   # (B, KG_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f10d89d-2d42-4bd7-9421-b27ec06cb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportDecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, locations):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ðŸ”‘ Location-specific output heads\n",
    "        self.heads = nn.ModuleDict({\n",
    "            loc: nn.Linear(hidden_dim, vocab_size)\n",
    "            for loc in locations\n",
    "        })\n",
    "\n",
    "    def forward(self, fused_feat, input_ids, locations):\n",
    "        \"\"\"\n",
    "        fused_feat: (B, HIDDEN_DIM)\n",
    "        input_ids: (B, T)\n",
    "        locations: list[str] length B\n",
    "        \"\"\"\n",
    "\n",
    "        emb = self.embedding(input_ids)          # (B, T, D)\n",
    "\n",
    "        h0 = fused_feat.unsqueeze(0)             # (1, B, H)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "\n",
    "        out, _ = self.lstm(emb, (h0, c0))        # (B, T, H)\n",
    "\n",
    "        # ðŸ”¥ Apply correct head PER SAMPLE\n",
    "        logits = torch.zeros(\n",
    "            out.size(0), out.size(1), self.heads[locations[0]].out_features,\n",
    "            device=out.device\n",
    "        )\n",
    "\n",
    "        for i, loc in enumerate(locations):\n",
    "            logits[i] = self.heads[loc](out[i])\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddfd46a5-1f0c-4a92-a8c7-4b1a6167f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ReportDecoderLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    locations=list(LOCATION_TOKENS.keys())\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ed8412b-a828-4a8d-82f4-d69ad7380979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN initialized\n"
     ]
    }
   ],
   "source": [
    "KG_IN_DIM = X_nodes.shape[1]   # number of node features\n",
    "KG_HIDDEN_DIM = 256            # you can tune this\n",
    "KG_OUT_DIM = EMBED_DIM         # must match fusion input\n",
    "\n",
    "gcn = GCN(\n",
    "    in_dim=KG_IN_DIM,\n",
    "    hidden_dim=KG_HIDDEN_DIM,\n",
    "    out_dim=KG_OUT_DIM,\n",
    "    num_layers=2\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"GCN initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dfb10be-ab46-4d5c-beb1-1a5207ab7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORBIDDEN = {\n",
    "    \"Head\": [\n",
    "        # Thorax\n",
    "        \"lung\", \"lobe\", \"bronchus\", \"pleura\", \"pulmonary\",\n",
    "        \"heart\", \"cardiac\", \"pericardium\",\n",
    "        \"aorta\", \"ivc\", \"svc\",\n",
    "\n",
    "        # Abdomen\n",
    "        \"liver\", \"hepatic\", \"spleen\", \"pancreas\",\n",
    "        \"bowel\", \"colon\", \"jejunum\", \"ileum\", \"duodenum\",\n",
    "\n",
    "        # GU\n",
    "        \"kidney\", \"renal\", \"ureter\", \"bladder\",\n",
    "        \"uterus\", \"ovary\", \"prostate\"\n",
    "    ],\n",
    "\n",
    "    \"Thorax\": [\n",
    "        # Abdomen\n",
    "        \"liver\", \"hepatic\", \"spleen\", \"pancreas\",\n",
    "        \"bowel\", \"colon\", \"jejunum\", \"ileum\", \"duodenum\",\n",
    "\n",
    "        # GU\n",
    "        \"kidney\", \"renal\", \"ureter\", \"bladder\",\n",
    "        \"uterus\", \"ovary\", \"prostate\"\n",
    "    ],\n",
    "\n",
    "    \"Abdomen\": [\n",
    "        # Thorax\n",
    "        \"lung\", \"lobe\", \"bronchus\", \"pleura\",\n",
    "        \"heart\", \"cardiac\", \"pericardium\",\n",
    "\n",
    "        # Neuro / Head\n",
    "        \"brain\", \"cerebral\", \"ventricle\",\n",
    "        \"orbit\", \"parotid\", \"thyroid\"\n",
    "    ],\n",
    "\n",
    "    \"Spine and Muscles\": [\n",
    "        # Thorax\n",
    "        \"lung\", \"pleura\", \"heart\", \"pericardium\",\n",
    "\n",
    "        # Abdomen\n",
    "        \"liver\", \"hepatic\", \"spleen\", \"pancreas\",\n",
    "        \"bowel\", \"colon\",\n",
    "\n",
    "        # GU\n",
    "        \"kidney\", \"renal\", \"ureter\", \"bladder\",\n",
    "        \"uterus\", \"ovary\", \"prostate\"\n",
    "    ],\n",
    "\n",
    "    \"Reproductive and Urinary System\": [\n",
    "        # Thorax\n",
    "        \"lung\", \"lobe\", \"bronchus\", \"pleura\",\n",
    "        \"heart\", \"pericardium\",\n",
    "\n",
    "        # Neuro / Head\n",
    "        \"brain\", \"cerebral\", \"ventricle\",\n",
    "        \"spinal cord\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c956511-33ed-48b0-bd79-b53ff7667f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49f40052-4425-4983-8b0f-596134f2a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all encoders and fusion\n",
    "freeze_module(ct_encoder)\n",
    "freeze_module(mri_encoder)\n",
    "freeze_module(text_encoder)\n",
    "freeze_module(gcn)\n",
    "freeze_module(fusion)\n",
    "\n",
    "# Ensure decoder is trainable\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee472c5d-d1f5-444a-a0c3-00c8bc3106fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "params = [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19846779-60db-4b59-b9ac-9730ea6eda58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT Encoder: 0 trainable params\n",
      "MRI Encoder: 0 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 0 trainable params\n",
      "Fusion: 0 trainable params\n",
      "Decoder: 87,710,553 trainable params\n"
     ]
    }
   ],
   "source": [
    "def count_trainable(name, module):\n",
    "    n = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    print(f\"{name}: {n:,} trainable params\")\n",
    "\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b52a620b-10fb-436e-970f-f0e939a345ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(train_loader):\n",
    "    decoder.train()\n",
    "    \n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "    fusion.eval()\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Training\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )  # (B, HIDDEN_DIM)\n",
    "\n",
    "        # ======================================================\n",
    "        # ðŸ”‘ Inject LOCATION-SPECIFIC BOS (PRESERVED)\n",
    "        # ======================================================\n",
    "        B = report_ids.size(0)\n",
    "\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)  # (B, 1)\n",
    "\n",
    "        # Prepend BOS to reports\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        # ======================================================\n",
    "        # ðŸ”¥ Teacher forcing (CORRECT SHIFT)\n",
    "        # ======================================================\n",
    "        decoder_inputs = report_ids[:, :-1]   # includes BOS\n",
    "        targets = report_ids[:, 1:]           # next-token targets\n",
    "\n",
    "        logits = decoder(\n",
    "            fused_feat,\n",
    "            decoder_inputs,\n",
    "            locations\n",
    "        )\n",
    "\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c2f4935-6cd1-4dec-ab73-b2fabdb29622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.67it/s, loss=7.4073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 7.8797 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.66it/s, loss=5.0432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | Train Loss: 6.1610 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.66it/s, loss=5.5728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | Train Loss: 5.8749 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:39<00:00,  4.06it/s, loss=5.2977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | Train Loss: 5.6311 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:40<00:00,  3.98it/s, loss=5.8605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | Train Loss: 5.4052 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.88it/s, loss=4.7466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | Train Loss: 5.1918 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.79it/s, loss=5.5792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | Train Loss: 5.0731 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.83it/s, loss=5.7146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | Train Loss: 4.9675 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=5.3108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | Train Loss: 4.8507 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.77it/s, loss=4.3164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | Train Loss: 4.7479 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=4.4691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 | Train Loss: 4.6430 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=4.7357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 | Train Loss: 4.5953 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.85it/s, loss=5.5242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 | Train Loss: 4.5440 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.74it/s, loss=5.3071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 | Train Loss: 4.4798 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.71it/s, loss=4.1264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 | Train Loss: 4.4291 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 15\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3e5ce2e-48a8-451e-9d20-8f181e02d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_report(\n",
    "    fused_feat,\n",
    "    location,\n",
    "    max_len=MAX_REPORT_LEN,\n",
    "    min_len=20\n",
    "):\n",
    "    decoder.eval()\n",
    "\n",
    "    bos_id = tokenizer.convert_tokens_to_ids(\n",
    "        LOCATION_TOKENS[location][\"bos\"]\n",
    "    )\n",
    "    eos_id = tokenizer.convert_tokens_to_ids(\n",
    "        LOCATION_TOKENS[location][\"eos\"]\n",
    "    )\n",
    "\n",
    "    generated_ids = [bos_id]\n",
    "    word_count = 0\n",
    "\n",
    "    for _ in range(max_len * 2):\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            generated_ids,\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)   # (1, T)\n",
    "\n",
    "        # ðŸ”‘ CORRECT decoder call\n",
    "        logits = decoder(\n",
    "            fused_feat,          # (1, HIDDEN_DIM)\n",
    "            input_ids,           # (1, T)\n",
    "            [location]           # list[str] of length 1\n",
    "        )  # (1, T, vocab)\n",
    "\n",
    "        step_logits = logits[0, -1]  # (vocab,)\n",
    "\n",
    "        # ðŸ”’ Apply forbidden-token masking\n",
    "        forbidden_words = FORBIDDEN[location]\n",
    "        forbidden_ids = tokenizer.convert_tokens_to_ids(forbidden_words)\n",
    "        forbidden_ids = [i for i in forbidden_ids if i != tokenizer.unk_token_id]\n",
    "\n",
    "        step_logits[forbidden_ids] = -1e9\n",
    "\n",
    "        next_id = torch.argmax(step_logits).item()\n",
    "\n",
    "        if next_id == eos_id and word_count < min_len:\n",
    "            continue\n",
    "\n",
    "        generated_ids.append(next_id)\n",
    "        word_count += 1\n",
    "\n",
    "        if next_id == eos_id or word_count >= max_len:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(\n",
    "#     fused_feat,\n",
    "#     location,\n",
    "#     max_len=MAX_REPORT_LEN,\n",
    "#     min_len=20\n",
    "# ):\n",
    "#     decoder.eval()\n",
    "\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"bos\"]\n",
    "#     )\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"eos\"]\n",
    "#     )\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "#     word_count = 0\n",
    "\n",
    "#     for _ in range(max_len * 2):\n",
    "\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)   # (1, T)\n",
    "\n",
    "#         logits = decoder(\n",
    "#             fused_feat,\n",
    "#             decoder_inputs,\n",
    "#             locations\n",
    "#         )\n",
    "\n",
    "#         forbidden_words = FORBIDDEN[location]\n",
    "#         forbidden_ids = tokenizer.convert_tokens_to_ids(forbidden_words)\n",
    "#         logits[forbidden_ids] = -1e9\n",
    "\n",
    "#         next_id = torch.argmax(logits[0, -1]).item()\n",
    "\n",
    "#         if next_id == eos_id and word_count < min_len:\n",
    "#             continue\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "#         word_count += 1\n",
    "\n",
    "#         if next_id == eos_id or word_count >= max_len:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04ff3156-f2d5-4174-9355-c599eb2349e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(test_loader):\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "    fusion.eval()\n",
    "    decoder.eval()   # âœ… GRU decoder only\n",
    "\n",
    "    generated_reports = []\n",
    "    ground_truth_reports = []\n",
    "    locations_all = []\n",
    "\n",
    "    pbar = tqdm(\n",
    "        test_loader,\n",
    "        desc=\"Generating reports\",\n",
    "        total=len(test_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"]   # CPU OK\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feats = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )   # (B, HIDDEN_DIM)\n",
    "\n",
    "        # =========================\n",
    "        # Generate per sample\n",
    "        # =========================\n",
    "        for i in range(fused_feats.size(0)):\n",
    "\n",
    "            gen_report = generate_report(\n",
    "                fused_feats[i].unsqueeze(0),\n",
    "                locations[i]\n",
    "            )\n",
    "\n",
    "            # ==================================================\n",
    "            # ðŸ”‘ Align GT format with training (KEEP THIS)\n",
    "            # ==================================================\n",
    "            bos_token = LOCATION_TOKENS[locations[i]][\"bos\"]\n",
    "            gt_report = bos_token + \" \" + tokenizer.decode(\n",
    "                report_ids[i],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            generated_reports.append(gen_report)\n",
    "            ground_truth_reports.append(gt_report)\n",
    "            locations_all.append(locations[i])\n",
    "\n",
    "    return generated_reports, ground_truth_reports, locations_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05f64d93-576a-469e-afb2-514c63f9c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2a24711-5da5-482e-bceb-d6b1fa89ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\multihead\\generated_vs_gt_reports_decoder_only_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\multihead\\generated_vs_gt_reports_decoder_only_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8afed621-58d0-44aa-8620-5df2c6253752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STAGE 2: Unfreeze Fusion\n",
    "# =========================\n",
    "\n",
    "# Keep encoders frozen\n",
    "freeze_module(ct_encoder)\n",
    "freeze_module(mri_encoder)\n",
    "freeze_module(text_encoder)\n",
    "freeze_module(gcn)\n",
    "\n",
    "# Unfreeze fusion\n",
    "for p in fusion.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Decoder already trainable\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d512f5a8-bea7-44a0-a587-5d72288256c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trainable Parameters Check ===\n",
      "CT Encoder: 0 trainable params\n",
      "MRI Encoder: 0 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 0 trainable params\n",
      "Fusion: 524,800 trainable params\n",
      "Decoder: 87,710,553 trainable params\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Trainable Parameters Check ===\")\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f03aa75-e2ad-4ab5-8def-19d217b6357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-2 optimizer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Optimizer for Stage 2\n",
    "# =========================\n",
    "\n",
    "stage2_params = []\n",
    "\n",
    "stage2_params += [p for p in fusion.parameters() if p.requires_grad]\n",
    "stage2_params += [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    stage2_params,\n",
    "    lr=1e-4,          # ðŸ”‘ LOWER LR\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "print(\"Stage-2 optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "228ca87f-834d-4984-ae4e-1f0bf1f7c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_stage2(train_loader):\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Stage-2 Training (Fusion + Decoder)\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        with torch.no_grad():\n",
    "            ct_feats = ct_encoder(ct_imgs)\n",
    "            mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "            ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "            mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "            text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "            kg_feat = get_kg_embeddings(\n",
    "                locations, gcn, X_nodes, A_hat_dict\n",
    "            )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # Location-specific BOS\n",
    "        # =========================\n",
    "        B = report_ids.size(0)\n",
    "\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        decoder_inputs = report_ids[:, :-1]\n",
    "        targets = report_ids[:, 1:]\n",
    "        \n",
    "        logits = decoder(\n",
    "            fused_feat,\n",
    "            decoder_inputs,\n",
    "            locations\n",
    "        )\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b8adbdd-ed1a-49f4-ae84-8fb3175aae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.75it/s, loss=3.3447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 1/10 | Loss: 4.4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=3.7776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 2/10 | Loss: 4.3188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.61it/s, loss=3.6472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 3/10 | Loss: 4.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=4.8699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 4/10 | Loss: 4.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.71it/s, loss=4.0666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 5/10 | Loss: 4.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.80it/s, loss=4.9297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 6/10 | Loss: 4.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.77it/s, loss=2.9259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 7/10 | Loss: 3.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.70it/s, loss=3.5885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 8/10 | Loss: 3.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=3.7411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 9/10 | Loss: 3.8954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.62it/s, loss=3.6983]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 10/10 | Loss: 3.8623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STAGE2_EPOCHS = 10\n",
    "\n",
    "for epoch in range(STAGE2_EPOCHS):\n",
    "    loss = train_one_epoch_stage2(train_loader)\n",
    "    print(\n",
    "        f\"[Stage-2] Epoch {epoch+1}/{STAGE2_EPOCHS} | \"\n",
    "        f\"Loss: {loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc91865a-c00e-4d9e-8b09-58c2ba96ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6488cdbd-fecf-47a3-8daa-aae74026195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\multihead\\generated_vs_gt_reports_fusion_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\multihead\\generated_vs_gt_reports_fusion_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be851ca-1f94-4992-9659-3d0e07a283ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIzaSyC0Xq7VNHUQDREmLB7DP97tsxz0SmKYPwQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c959554-9359-4537-9779-760f6cd954eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: google-generativeai 0.8.6\n",
      "Uninstalling google-generativeai-0.8.6:\n",
      "  Successfully uninstalled google-generativeai-0.8.6\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting google-genai\n",
      "  Downloading google_genai-1.56.0-py3-none-any.whl.metadata (53 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai) (2.45.0)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-genai) (2.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-genai) (2.32.5)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-genai) (4.15.0)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\student\\appdata\\roaming\\python\\python312\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.56.0-py3-none-any.whl (426 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, distro, httpx, google-genai\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [distro]\n",
      "  Attempting uninstall: httpx\n",
      "   ---------- ----------------------------- 1/4 [distro]\n",
      "    Found existing installation: httpx 0.27.2\n",
      "   ---------- ----------------------------- 1/4 [distro]\n",
      "    Uninstalling httpx-0.27.2:\n",
      "   ---------- ----------------------------- 1/4 [distro]\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "   ---------- ----------------------------- 1/4 [distro]\n",
      "   -------------------- ------------------- 2/4 [httpx]\n",
      "   -------------------- ------------------- 2/4 [httpx]\n",
      "   -------------------- ------------------- 2/4 [httpx]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ------------------------------ --------- 3/4 [google-genai]\n",
      "   ---------------------------------------- 4/4 [google-genai]\n",
      "\n",
      "Successfully installed distro-1.9.0 google-genai-1.56.0 httpx-0.28.1 tenacity-9.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script distro.exe is installed in 'C:\\Users\\Student\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\Student\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fief-client 0.20.0 requires httpx<0.28.0,>=0.21.3, but you have httpx 0.28.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install --quiet google-generativeai\n",
    "!pip uninstall -y google-generativeai\n",
    "!pip install -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "683b4c45-36d6-4157-8ac6-fdde65ba9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# # Option 1: Direct (NOT recommended for shared systems)\n",
    "# GEMINI_API_KEY = \"AIzaSyC0Xq7VNHUQDREmLB7DP97tsxz0SmKYPwQ\"\n",
    "\n",
    "# # Option 2: Environment variable (recommended)\n",
    "# # os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "# Recommended even on shared systems (Jupyter-safe)\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyC0Xq7VNHUQDREmLB7DP97tsxz0SmKYPwQ\"\n",
    "\n",
    "client = genai.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "42a2b7e1-7e65-4976-ae7d-445e30570f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = genai.GenerativeModel(\n",
    "#     model_name=\"gemini-1.5-pro\",\n",
    "#     generation_config={\n",
    "#         \"temperature\": 0.1,        # LOW creativity\n",
    "#         \"top_p\": 0.9,\n",
    "#         \"top_k\": 40,\n",
    "#         \"max_output_tokens\": 512\n",
    "#     }\n",
    "# )\n",
    "\n",
    "MODEL_NAME = \"models/gemini-2.5-flash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3091da9a-9ef0-4af1-8974-3b14abe64e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"\"\"\n",
    "# You are a clinical radiology report refinement assistant.\n",
    "\n",
    "# STRICT RULES:\n",
    "# 1. DO NOT add new medical findings.\n",
    "# 2. DO NOT infer diseases, abnormalities, or diagnoses.\n",
    "# 3. DO NOT speculate or guess missing information.\n",
    "# 4. DO NOT introduce anatomy or observations not explicitly present.\n",
    "# 5. DO NOT change clinical meaning.\n",
    "# 6. Preserve negations exactly (e.g., \"no evidence of\").\n",
    "# 7. If the input is vague or incomplete, keep it vague.\n",
    "# 8. If unsure, return the original text unchanged.\n",
    "\n",
    "# ALLOWED:\n",
    "# - Improve grammar and clarity\n",
    "# - Improve clinical phrasing\n",
    "# - Remove redundancy\n",
    "# - Improve sentence flow\n",
    "# - Standardize terminology\n",
    "\n",
    "# If any rule conflicts, prioritize safety and factual consistency.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# SYSTEM_PROMPT = \"\"\"\n",
    "# You are a clinical radiology report refinement assistant.\n",
    "\n",
    "# STRICT RULES (MUST FOLLOW):\n",
    "# 1. DO NOT add new findings, diseases, abnormalities, or diagnoses.\n",
    "# 2. DO NOT infer, speculate, or assume missing information.\n",
    "# 3. DO NOT introduce new anatomy or observations not explicitly present.\n",
    "# 4. DO NOT change the clinical meaning or certainty of any statement.\n",
    "# 5. Preserve all negations exactly (e.g., â€œno evidence ofâ€, â€œabsence ofâ€).\n",
    "# 6. If information is missing or unclear, keep it unchanged and do not elaborate.\n",
    "# 7. If refinement would violate any rule, return the original text verbatim.\n",
    "\n",
    "# LENGTH AND COMPLETENESS CONSTRAINTS:\n",
    "# 8. The refined report MUST contain at least 50 words.\n",
    "# 9. If the original report is shorter, you may:\n",
    "#    - Rephrase sentences\n",
    "#    - Combine or split sentences\n",
    "#    - Improve clinical phrasing\n",
    "#    - Reiterate existing information in a clearer medical style\n",
    "#    BUT you must NOT introduce new facts.\n",
    "# 10. The final sentence MUST be grammatically complete and end with proper punctuation.\n",
    "# 11. Do NOT leave any sentence unfinished or abruptly terminated.\n",
    "\n",
    "# ALLOWED OPERATIONS:\n",
    "# - Grammar correction\n",
    "# - Clinical phrasing improvement\n",
    "# - Redundancy removal or controlled repetition\n",
    "# - Sentence flow and coherence improvement\n",
    "# - Terminology standardization\n",
    "\n",
    "# OUTPUT FORMAT:\n",
    "# - Return ONLY the refined report text.\n",
    "# - Do NOT add headings, explanations, bullet points, or commentary.\n",
    "# \"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a clinical radiology report refinement assistant.\n",
    "\n",
    "STRICT SAFETY RULES (MUST FOLLOW):\n",
    "1. DO NOT add any findings, diseases, abnormalities, diagnoses, or anatomy\n",
    "   that are NOT explicitly present in the provided inputs.\n",
    "2. DO NOT infer, speculate, or assume missing information.\n",
    "3. DO NOT change the clinical meaning, certainty, or negation of any statement.\n",
    "4. Preserve all negations exactly (e.g., â€œno evidence ofâ€, â€œabsence ofâ€).\n",
    "5. If any refinement would violate these rules, return the original text verbatim.\n",
    "\n",
    "GROUND TRUTH USAGE RULE:\n",
    "6. If REFERENCE FINDINGS / GROUND TRUTH is provided, you MAY ONLY use\n",
    "   information explicitly stated in the ground truth to:\n",
    "   - Expand sentences\n",
    "   - Rephrase content\n",
    "   - Improve clarity and structure\n",
    "7. DO NOT introduce content that is not present in either the generated report\n",
    "   or the ground truth.\n",
    "\n",
    "LENGTH AND COMPLETENESS CONSTRAINTS (MANDATORY):\n",
    "8. The refined report MUST contain AT LEAST 50 words.\n",
    "9. If the generated report is shorter than 50 words, you MUST increase length\n",
    "   by rephrasing, restructuring, or elaborating ONLY USING GROUND TRUTH content.\n",
    "10. If no ground truth is provided and the report is under 50 words,\n",
    "    you may carefully rephrase or restate existing content WITHOUT adding facts.\n",
    "11. The final sentence MUST be grammatically complete and end with proper punctuation.\n",
    "12. DO NOT leave any sentence unfinished or abruptly terminated.\n",
    "\n",
    "ALLOWED OPERATIONS:\n",
    "- Grammar correction\n",
    "- Clinical phrasing improvement\n",
    "- Controlled repetition for clarity\n",
    "- Sentence flow and coherence improvement\n",
    "- Terminology standardization\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "- Return ONLY the refined report text.\n",
    "- Do NOT add headings, bullet points, explanations, or commentary.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8ddf925-40aa-4fa5-8870-5ee5b8611adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def refine_medical_report(\n",
    "#     generated_report: str,\n",
    "#     reference_findings: str = None\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     generated_report: model-generated report to refine\n",
    "#     reference_findings: OPTIONAL factual findings or labels (if available)\n",
    "#     \"\"\"\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# {SYSTEM_PROMPT}\n",
    "\n",
    "# ORIGINAL GENERATED REPORT:\n",
    "# \\\"\\\"\\\"\n",
    "# {generated_report}\n",
    "# \\\"\\\"\\\"\n",
    "# \"\"\"\n",
    "\n",
    "#     if reference_findings:\n",
    "#         prompt += f\"\"\"\n",
    "# REFERENCE FINDINGS (GROUND TRUTH â€“ DO NOT EXCEED):\n",
    "# \\\"\\\"\\\"\n",
    "# {reference_findings}\n",
    "# \\\"\\\"\\\"\n",
    "# \"\"\"\n",
    "\n",
    "#     prompt += \"\"\"\n",
    "# TASK:\n",
    "# Refine the ORIGINAL GENERATED REPORT while strictly following the rules.\n",
    "# Return ONLY the refined report text.\n",
    "# \"\"\"\n",
    "\n",
    "#     response = model.generate_content(prompt)\n",
    "#     return response.text.strip()\n",
    "\n",
    "\n",
    "def refine_medical_report(\n",
    "    generated_report: str,\n",
    "    reference_findings: str = None\n",
    "):\n",
    "    prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "ORIGINAL GENERATED REPORT:\n",
    "\\\"\\\"\\\"\n",
    "{generated_report}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "    if reference_findings:\n",
    "        prompt += f\"\"\"\n",
    "REFERENCE FINDINGS (GROUND TRUTH â€” DO NOT EXCEED):\n",
    "\\\"\\\"\\\"\n",
    "{reference_findings}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "    prompt += \"\\nRefine the report while strictly obeying the rules.\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"temperature\": 0.1,   # ðŸ”‘ low creativity\n",
    "            \"max_output_tokens\": 512,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "402d5d12-85ae-42be-b3eb-5b07fc29dd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is T2 hyperintense signal intensity noted within the right frontal lobe. Additionally, T\n"
     ]
    }
   ],
   "source": [
    "\n",
    "refined = refine_medical_report(generated_reports[19])\n",
    "print(refined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c8416beb-c619-4e04-8ea6-a74bddec6766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<HEAD_BOS> â€¢ heterogeneous signal lesion - mixed iso and hyperintense on t1wi â€¢ hyperintense on t2wi - in the region of the right lenticular nucleus, anterior limb of the right internal capsule, and external capsule â€¢ hypointense rim on t1wi that â€œ blooms â€ on t2wi â€¢ no mass effect â€¢ minimal enhancement seen post - gadolinium'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_reports[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "243b4c02-786c-497d-800f-b24cff5f8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-flash-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "for m in models:\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37546f23-59c4-4f12-bcd5-52a371befa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STAGE 3: Unfreeze GCN\n",
    "# =========================\n",
    "\n",
    "# Keep encoders frozen\n",
    "freeze_module(ct_encoder)\n",
    "freeze_module(mri_encoder)\n",
    "freeze_module(text_encoder)\n",
    "\n",
    "# Unfreeze GCN\n",
    "for p in gcn.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Fusion + Decoder remain trainable\n",
    "for p in fusion.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b702d08-add6-4b2e-8fbb-a86a7e6d9a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage-3 Trainable Params Check ===\n",
      "CT Encoder: 0 trainable params\n",
      "MRI Encoder: 0 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 1,192,448 trainable params\n",
      "Fusion: 524,800 trainable params\n",
      "Decoder: 25,056,837 trainable params\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Stage-3 Trainable Params Check ===\")\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "604e3a75-4514-40ac-8232-c55ec514f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-3 optimizer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Optimizer for Stage 3\n",
    "# =========================\n",
    "\n",
    "stage3_params = []\n",
    "\n",
    "stage3_params += [p for p in gcn.parameters() if p.requires_grad]\n",
    "stage3_params += [p for p in fusion.parameters() if p.requires_grad]\n",
    "stage3_params += [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    stage3_params,\n",
    "    lr=5e-5,          # ðŸ”‘ LOWER LR (KG is sensitive)\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "print(\"Stage-3 optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c8d8db3-7df9-40a5-a84a-a516bd89e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_stage3(train_loader):\n",
    "    gcn.train()\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Stage-3 Training (GCN + Fusion + Decoder)\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode frozen modalities\n",
    "        # =========================\n",
    "        with torch.no_grad():\n",
    "            ct_feats = ct_encoder(ct_imgs)\n",
    "            mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "            ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "            mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "            text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        # =========================\n",
    "        # GCN now TRAINABLE\n",
    "        # =========================\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # Location BOS + decoding\n",
    "        # =========================\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        decoder_inputs = report_ids[:, :-1]\n",
    "        targets = report_ids[:, 1:]\n",
    "\n",
    "        logits = decoder(\n",
    "            fused_feat,\n",
    "            decoder_inputs,\n",
    "            locations\n",
    "        )\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4308dc2b-d201-4b13-a79e-58a1ee231890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.61it/s, loss=3.0517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 1/10 | Loss: 3.4898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.78it/s, loss=3.4704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 2/10 | Loss: 3.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=2.4937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 3/10 | Loss: 3.4347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.79it/s, loss=3.5297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 4/10 | Loss: 3.4031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:38<00:00,  4.10it/s, loss=3.3280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 5/10 | Loss: 3.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.88it/s, loss=2.7290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 6/10 | Loss: 3.3389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:40<00:00,  3.94it/s, loss=2.7925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 7/10 | Loss: 3.3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=3.3145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 8/10 | Loss: 3.3207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.72it/s, loss=3.2163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 9/10 | Loss: 3.3245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.63it/s, loss=3.3888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 10/10 | Loss: 3.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STAGE3_EPOCHS = 10\n",
    "\n",
    "for epoch in range(STAGE3_EPOCHS):\n",
    "    loss = train_one_epoch_stage3(train_loader)\n",
    "    print(\n",
    "        f\"[Stage-3] Epoch {epoch+1}/{STAGE3_EPOCHS} | \"\n",
    "        f\"Loss: {loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42d9dd06-2eb2-4803-831f-f920c0f7cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aee280f3-2332-4c06-a944-4132a2cfc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_GCN_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_GCN_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "632301f9-74de-4e11-ac35-7d5cdf181778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helper: Unfreeze ResNet layer4 only\n",
    "# =========================\n",
    "\n",
    "def unfreeze_resnet_layer4(resnet_model):\n",
    "    # Freeze everything\n",
    "    for p in resnet_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Unfreeze layer4\n",
    "    for p in resnet_model.layer4.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Keep FC trainable (already replaced)\n",
    "    for p in resnet_model.fc.parameters():\n",
    "        p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15bd0d09-2e95-4777-a417-922177c2237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STAGE 4: Partial Image Unfreeze\n",
    "# =========================\n",
    "\n",
    "unfreeze_resnet_layer4(ct_encoder.cnn)\n",
    "unfreeze_resnet_layer4(mri_encoder.cnn)\n",
    "\n",
    "# Text encoder stays frozen\n",
    "freeze_module(text_encoder)\n",
    "\n",
    "# GCN + Fusion + Decoder remain trainable\n",
    "for p in gcn.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in fusion.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28b2b9a6-37c7-440b-8d8e-03cf223cd20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage-4 Trainable Params Check ===\n",
      "CT Encoder: 8,525,056 trainable params\n",
      "MRI Encoder: 8,525,056 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 1,192,448 trainable params\n",
      "Fusion: 524,800 trainable params\n",
      "Decoder: 25,056,837 trainable params\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Stage-4 Trainable Params Check ===\")\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29f42782-218d-43f4-856c-2bffcef86e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-4 optimizer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Optimizer for Stage 4\n",
    "# =========================\n",
    "\n",
    "stage4_params = []\n",
    "\n",
    "stage4_params += [p for p in ct_encoder.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in mri_encoder.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in gcn.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in fusion.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    stage4_params,\n",
    "    lr=1e-5,          # ðŸ”‘ VERY IMPORTANT: small LR\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "print(\"Stage-4 optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be5f7d04-4d8a-4331-98fe-dc33707ec07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_stage4(train_loader):\n",
    "    ct_encoder.train()\n",
    "    mri_encoder.train()\n",
    "    gcn.train()\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "\n",
    "    text_encoder.eval()   # still frozen\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Stage-4 Training (Visual + KG + Decoder)\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # Location BOS + decoding\n",
    "        # =========================\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        decoder_inputs = report_ids[:, :-1]\n",
    "        targets = report_ids[:, 1:]\n",
    "\n",
    "        logits = decoder(\n",
    "            fused_feat,\n",
    "            decoder_inputs,\n",
    "            locations\n",
    "        )\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1a4dd6f-6b12-4065-8617-b7b9f7d59502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.71it/s, loss=4.4528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 1/6 | Loss: 3.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.75it/s, loss=3.2478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 2/6 | Loss: 3.2968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=3.9971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 3/6 | Loss: 3.2943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.67it/s, loss=3.4107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 4/6 | Loss: 3.2878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.60it/s, loss=2.2344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 5/6 | Loss: 3.2694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:45<00:00,  3.52it/s, loss=2.0949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 6/6 | Loss: 3.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STAGE4_EPOCHS = 6\n",
    "\n",
    "for epoch in range(STAGE4_EPOCHS):\n",
    "    loss = train_one_epoch_stage4(train_loader)\n",
    "    print(\n",
    "        f\"[Stage-4] Epoch {epoch+1}/{STAGE4_EPOCHS} | \"\n",
    "        f\"Loss: {loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b556835-ff68-40ec-9da5-2d052abf33a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f60cb493-457a-4144-9428-1a1b21a6c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_encoder_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_encoder_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d433c8e-655c-4c1e-8312-62aa0c13a429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

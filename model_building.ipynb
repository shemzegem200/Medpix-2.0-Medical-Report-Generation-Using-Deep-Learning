{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e268d2-adfa-453a-a729-7618a96c3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchvision in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: transformers in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: numpy in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.9.1 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torchvision) (2.9.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: filelock in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from torch==2.9.1->torchvision) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ad9c51-ff2c-4eec-8a15-fce9dae20d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cpu\n",
      "torchvision: 0.24.1+cpu\n",
      "✅ torchvision fully loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "print(\"✅ torchvision fully loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf99192-6b64-436d-a50d-3ed9f5038408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\fyp_manish_shyam_phase2\\my_env\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b6c44f-c0c7-4e24-b881-5418ab7e4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_REPORT_LEN = 120\n",
    "EMBED_DIM = 256\n",
    "TEXT_DIM = 768\n",
    "KG_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "# NUM_WORKERS = 4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa24167f-d5e8-4528-8a88-4aaa0fbe550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Device count: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facda5f1-6fee-499a-99ab-fb18423a2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2fc8218-9260-411b-a11d-91c86cc8fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_TOKENS = {\n",
    "    \"Head\": {\n",
    "        \"bos\": \"<HEAD_BOS>\",\n",
    "        \"eos\": \"<HEAD_EOS>\"\n",
    "    },\n",
    "    \"Thorax\": {\n",
    "        \"bos\": \"<THORAX_BOS>\",\n",
    "        \"eos\": \"<THORAX_EOS>\"\n",
    "    },\n",
    "    \"Abdomen\": {\n",
    "        \"bos\": \"<ABDOMEN_BOS>\",\n",
    "        \"eos\": \"<ABDOMEN_EOS>\"\n",
    "    },\n",
    "    \"Spine and Muscles\": {\n",
    "        \"bos\": \"<SPINE_BOS>\",\n",
    "        \"eos\": \"<SPINE_EOS>\"\n",
    "    },\n",
    "    \"Reproductive and Urinary System\": {\n",
    "        \"bos\": \"<GU_BOS>\",\n",
    "        \"eos\": \"<GU_EOS>\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21dfa602-5591-400c-a31e-f4414251e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = {\n",
    "#     \"pad_token\": \"<PAD>\",\n",
    "#     \"additional_special_tokens\": []\n",
    "# }\n",
    "\n",
    "# for loc in LOCATION_TOKENS:\n",
    "#     special_tokens[\"additional_special_tokens\"].append(\n",
    "#         LOCATION_TOKENS[loc][\"bos\"]\n",
    "#     )\n",
    "#     special_tokens[\"additional_special_tokens\"].append(\n",
    "#         LOCATION_TOKENS[loc][\"eos\"]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd7cfad-b0d8-4161-83d6-122e39909432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sacremoses in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: protobuf in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (6.33.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: regex in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from sacremoses) (1.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from sacremoses) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from click->sacremoses) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses protobuf sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98215c67-208f-4dc4-84c4-31d3f4d4909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 57727\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# VOCAB_SIZE = len(tokenizer)\n",
    "# print(\"Vocab size:\", VOCAB_SIZE)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": []\n",
    "}\n",
    "\n",
    "for loc in LOCATION_TOKENS:\n",
    "    special_tokens[\"additional_special_tokens\"].append(\n",
    "        LOCATION_TOKENS[loc][\"bos\"]\n",
    "    )\n",
    "    special_tokens[\"additional_special_tokens\"].append(\n",
    "        LOCATION_TOKENS[loc][\"eos\"]\n",
    "    )\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6d2785-65ea-4f89-9b06-69354eecc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: pillow in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pillow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394cbe35-d47e-48ff-a4b8-e3fb761c3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MedPixDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Normalize NaNs early (VERY IMPORTANT)\n",
    "        self.df = self.df.fillna(\"\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def load_image(self, img_path):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def parse_image_list(self, s):\n",
    "        # CSV stores lists as strings: \"['path1', 'path2']\"\n",
    "        if s == \"\" or s == \"[]\":\n",
    "            return []\n",
    "        return ast.literal_eval(s)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -------- Images --------\n",
    "        ct_images = []\n",
    "        mri_images = []\n",
    "\n",
    "        ct_paths = self.parse_image_list(row[\"CT_image_paths\"])\n",
    "        mri_paths = self.parse_image_list(row[\"MRI_image_paths\"])\n",
    "\n",
    "        for p in ct_paths:\n",
    "            ct_images.append(self.load_image(p))\n",
    "\n",
    "        for p in mri_paths:\n",
    "            mri_images.append(self.load_image(p))\n",
    "\n",
    "        # -------- Text Encoder Input --------\n",
    "        text_input = row[\"combined_text\"]\n",
    "\n",
    "        # -------- Target Report --------\n",
    "        report = row[\"findings\"]\n",
    "\n",
    "        # -------- Location (KG routing) --------\n",
    "        location_category = row[\"Location Category\"]\n",
    "\n",
    "        return {\n",
    "            \"uid\": row[\"U_id\"],\n",
    "            \"ct_images\": ct_images,     # list[Tensor]\n",
    "            \"mri_images\": mri_images,   # list[Tensor]\n",
    "            \"text_input\": text_input,   # str\n",
    "            \"report\": report,           # str\n",
    "            \"location\": location_category\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252ca940-92c8-4b9c-8494-74ab73f4afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch items contain:\n",
    "    - ct_images: list[Tensor]\n",
    "    - mri_images: list[Tensor]\n",
    "    - text_input: str\n",
    "    - report: str\n",
    "    - location: str\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- CT images ----------\n",
    "    max_ct = max(len(b[\"ct_images\"]) for b in batch)\n",
    "    ct_imgs, ct_masks = [], []\n",
    "\n",
    "    for b in batch:\n",
    "        imgs = b[\"ct_images\"]\n",
    "        if len(imgs) == 0:\n",
    "            dummy = torch.zeros(3, 224, 224)\n",
    "            imgs = [dummy]\n",
    "\n",
    "        pad = max_ct - len(imgs)\n",
    "        imgs = imgs + [torch.zeros_like(imgs[0])] * pad\n",
    "        mask = [1] * (len(imgs) - pad) + [0] * pad\n",
    "\n",
    "        ct_imgs.append(torch.stack(imgs))\n",
    "        ct_masks.append(torch.tensor(mask))\n",
    "\n",
    "    ct_imgs = torch.stack(ct_imgs)      # (B, N_ct, 3, H, W)\n",
    "    ct_masks = torch.stack(ct_masks)    # (B, N_ct)\n",
    "\n",
    "    # ---------- MRI images ----------\n",
    "    max_mri = max(len(b[\"mri_images\"]) for b in batch)\n",
    "    mri_imgs, mri_masks = [], []\n",
    "\n",
    "    for b in batch:\n",
    "        imgs = b[\"mri_images\"]\n",
    "        if len(imgs) == 0:\n",
    "            dummy = torch.zeros(3, 224, 224)\n",
    "            imgs = [dummy]\n",
    "\n",
    "        pad = max_mri - len(imgs)\n",
    "        imgs = imgs + [torch.zeros_like(imgs[0])] * pad\n",
    "        mask = [1] * (len(imgs) - pad) + [0] * pad\n",
    "\n",
    "        mri_imgs.append(torch.stack(imgs))\n",
    "        mri_masks.append(torch.tensor(mask))\n",
    "\n",
    "    mri_imgs = torch.stack(mri_imgs)    # (B, N_mri, 3, H, W)\n",
    "    mri_masks = torch.stack(mri_masks)  # (B, N_mri)\n",
    "\n",
    "    # ---------- Text encoder input ----------\n",
    "    text_inputs = [b[\"text_input\"] for b in batch]\n",
    "    text_enc = tokenizer(\n",
    "        text_inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------- Decoder target ----------\n",
    "    reports = [b[\"report\"] for b in batch]   # <-- RAW ground truth\n",
    "    report_enc = tokenizer(\n",
    "        reports,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_REPORT_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------- Location ----------\n",
    "    locations = [b[\"location\"] for b in batch]\n",
    "\n",
    "    return {\n",
    "        \"ct_images\": ct_imgs,\n",
    "        \"ct_masks\": ct_masks,\n",
    "        \"mri_images\": mri_imgs,\n",
    "        \"mri_masks\": mri_masks,\n",
    "        \"text_input_ids\": text_enc[\"input_ids\"],\n",
    "        \"text_attention_mask\": text_enc[\"attention_mask\"],\n",
    "        \"report_input_ids\": report_enc[\"input_ids\"],\n",
    "        \"report_attention_mask\": report_enc[\"attention_mask\"],\n",
    "        \"reports\": reports,              # <-- RAW ground-truth strings\n",
    "        \"locations\": locations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dc75b45-9a39-40a0-9423-4e653372bc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy<2 in c:\\fyp_manish_shyam_phase2\\my_env\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba75f613-6401-4ecb-aed9-f697e9c3854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "Torch: 2.9.1+cpu\n",
      "Torchvision: 0.24.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6311cd02-d41d-48b2-ab18-b72fe0451f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPX1009\n",
      "CT images: 2\n",
      "MRI images: 0\n",
      "Text length: 379\n",
      "Report length: 152\n",
      "Location: Reproductive and Urinary System\n"
     ]
    }
   ],
   "source": [
    "ds = MedPixDataset(r\"C:\\fyp_manish_shyam_phase2\\data\\df_overall.csv\", transform=image_transform)\n",
    "\n",
    "sample = ds[0]\n",
    "print(sample[\"uid\"])\n",
    "print(\"CT images:\", len(sample[\"ct_images\"]))\n",
    "print(\"MRI images:\", len(sample[\"mri_images\"]))\n",
    "print(\"Text length:\", len(sample[\"text_input\"]))\n",
    "print(\"Report length:\", len(sample[\"report\"]))\n",
    "print(\"Location:\", sample[\"location\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "949bc98e-c4fe-4b56-ab60-b52f9a9017f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda71f90-3260-4a5e-b38b-1a9e4773b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT images shape: torch.Size([3, 2, 3, 224, 224])\n",
      "CT masks shape: torch.Size([3, 2])\n",
      "MRI images shape: torch.Size([3, 1, 3, 224, 224])\n",
      "MRI masks shape: torch.Size([3, 2])\n",
      "Text input ids shape: torch.Size([3, 80])\n",
      "Report input ids shape: torch.Size([3, 65])\n",
      "Raw reports count: 3\n",
      "First report preview:\n",
      " Bladder with thickened wall and diverticulum on the right. Diverticulum is mostly likely secondary to chronic outflow obstruction. Prostate enlargement.\n",
      "Locations: ['Reproductive and Urinary System', 'Thorax', 'Reproductive and Urinary System']\n"
     ]
    }
   ],
   "source": [
    "# Taking 2–3 samples manually for doing a small sanity check\n",
    "batch_samples = [ds[i] for i in range(3)]\n",
    "\n",
    "batch = collate_fn(batch_samples)\n",
    "print(\"CT images shape:\", batch[\"ct_images\"].shape)\n",
    "print(\"CT masks shape:\", batch[\"ct_masks\"].shape)\n",
    "\n",
    "print(\"MRI images shape:\", batch[\"mri_images\"].shape)\n",
    "print(\"MRI masks shape:\", batch[\"mri_masks\"].shape)\n",
    "\n",
    "print(\"Text input ids shape:\", batch[\"text_input_ids\"].shape)\n",
    "print(\"Report input ids shape:\", batch[\"report_input_ids\"].shape)\n",
    "\n",
    "print(\"Raw reports count:\", len(batch[\"reports\"]))\n",
    "print(\"First report preview:\\n\", batch[\"reports\"][0][:200])\n",
    "\n",
    "print(\"Locations:\", batch[\"locations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0001bd89-f0fe-4dd5-b337-c529ddfd423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 671\n",
      "Train samples: 637\n",
      "Test samples:  34\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# ---- Load full dataset ----\n",
    "full_dataset = MedPixDataset(\n",
    "    r\"C:\\fyp_manish_shyam_phase2\\data\\df_overall.csv\",\n",
    "    transform=image_transform\n",
    ")\n",
    "\n",
    "# ---- 80 / 20 split ----\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.95 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {dataset_size}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples:  {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b688522-5b96-4c03-b666-61dbafe7520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & Test loaders ready\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn\n",
    "    # , pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,          # No shuffle for test\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn\n",
    "    # , pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train & Test loaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "573ecc90-ab9e-40d2-b629-720770eeeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        for p in base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        base.fc = nn.Linear(base.fc.in_features, embed_dim)\n",
    "        self.cnn = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, 3, H, W)\n",
    "        return: (B, N, D)\n",
    "        \"\"\"\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "        feats = self.cnn(x)\n",
    "        feats = feats.view(B, N, -1)\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f6df810-7bae-41ac-92c1-78f1c5838f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_encoder = ImageEncoder().to(DEVICE)\n",
    "mri_encoder = ImageEncoder().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "444b5b38-d67b-4b11-9f60-49ab57ce0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_mean_pooling(feats, masks):\n",
    "    masks = masks.unsqueeze(-1).float()   # (B, N, 1)\n",
    "    summed = (feats * masks).sum(dim=1)\n",
    "    denom = masks.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4ae7796-84ff-40a8-b22d-058f6ec9d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "\n",
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.proj = nn.Linear(TEXT_DIM, EMBED_DIM)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         cls = out.last_hidden_state[:, 0]  # CLS token\n",
    "#         return self.proj(cls)\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, embed_dim):\n",
    "        super().__init__()\n",
    "        self.lm = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        hidden_dim = self.lm.config.hidden_size\n",
    "        self.proj = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Mean pooling over tokens (causal models do NOT have CLS)\n",
    "        hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "\n",
    "        pooled = (hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        return self.proj(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6270bce5-2b14-4bc7-8433-d5ef8864ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6060e7099dc4d08a519963ad7d88854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\fyp_manish_shyam_phase2\\my_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\HF_CACHE\\models--microsoft--BioGPT-Large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c542ed7c8884ea39bf1b90b9d540955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/6.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEXT_MODEL_NAME = \"microsoft/BioGPT-Large\"\n",
    "\n",
    "text_encoder = TextEncoder(\n",
    "    model_name=TEXT_MODEL_NAME,\n",
    "    embed_dim=EMBED_DIM\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7a96d06-ff22-4b07-a163-ea2c4ebccf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BioGptScaledWordEmbedding(57727, 1600, padding_idx=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.lm.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f58656a4-447a-43af-9d8d-42dbf8ed7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, A_hat, X):\n",
    "        \"\"\"\n",
    "        A_hat: (N, N) normalized adjacency\n",
    "        X: (N, D)\n",
    "        \"\"\"\n",
    "        return F.relu(self.linear(A_hat @ X))\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(GCNLayer(dims[i], dims[i + 1]))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, A_hat, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(A_hat, X)\n",
    "        return X.mean(dim=0)   # graph-level embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1794a05c-21ea-42e7-9cbd-2384aebb02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A: (N, N) raw adjacency matrix\n",
    "    returns: (N, N) normalized adjacency with self-loops\n",
    "    \"\"\"\n",
    "    device = A.device\n",
    "    N = A.size(0)\n",
    "\n",
    "    # Add self-loops\n",
    "    A_tilde = A + torch.eye(N, device=device)\n",
    "\n",
    "    # Degree\n",
    "    D = A_tilde.sum(dim=1)\n",
    "\n",
    "    # D^{-1/2}\n",
    "    D_inv_sqrt = torch.pow(D, -0.5)\n",
    "    D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = torch.diag(D_inv_sqrt)\n",
    "\n",
    "    # Symmetric normalization\n",
    "    A_hat = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "    return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "555d53c6-1166-4d3c-8a54-3df126fc4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7178b251f4d47ada4439384a1e5907a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head: A_hat shape = torch.Size([4400, 4400])\n",
      "Thorax: A_hat shape = torch.Size([4400, 4400])\n",
      "Abdomen: A_hat shape = torch.Size([4400, 4400])\n",
      "Spine and Muscles: A_hat shape = torch.Size([4400, 4400])\n",
      "Reproductive and Urinary System: A_hat shape = torch.Size([4400, 4400])\n",
      "Shared X_nodes shape: torch.Size([4400, 4400])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "KG_LOCATION_MAP = {\n",
    "    \"Head\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Head_matrix.csv\",\n",
    "    \"Thorax\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Thorax_matrix.csv\",\n",
    "    \"Abdomen\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Abdomen_matrix.csv\",\n",
    "    \"Spine and Muscles\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Spine_and_Muscles_matrix.csv\",\n",
    "    \"Reproductive and Urinary System\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Reproductive_and_Urinary_System_matrix.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "A_hat_dict = {}\n",
    "\n",
    "# ---- Load and normalize adjacency matrices ----\n",
    "for loc, path in KG_LOCATION_MAP.items():\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    A = torch.tensor(\n",
    "        df.values,\n",
    "        dtype=torch.float32,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    A_hat = normalize_adjacency(A)\n",
    "    A_hat_dict[loc] = A_hat\n",
    "\n",
    "    print(f\"{loc}: A_hat shape = {A_hat.shape}\")\n",
    "\n",
    "# ---- Create shared X_nodes (identity) ----\n",
    "# Node count inferred from any adjacency matrix\n",
    "example_loc = next(iter(A_hat_dict))\n",
    "N_nodes = A_hat_dict[example_loc].shape[0]\n",
    "\n",
    "X_nodes = torch.eye(N_nodes, device=DEVICE)\n",
    "\n",
    "print(\"Shared X_nodes shape:\", X_nodes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "683603fc-db9c-4183-b56e-bfd3b0c5171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embed_dim * 4, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, ct_feat, mri_feat, text_feat, kg_feat):\n",
    "        \"\"\"\n",
    "        All inputs: (B, EMBED_DIM)\n",
    "        Output: (B, HIDDEN_DIM)\n",
    "        \"\"\"\n",
    "        fused = torch.cat(\n",
    "            [ct_feat, mri_feat, text_feat, kg_feat],\n",
    "            dim=-1\n",
    "        )\n",
    "        fused = self.dropout(fused)\n",
    "        return self.fc(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da79e065-95d6-4b57-bd4c-5ee9aa682688",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = FeatureFusion().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acbf9208-3be2-4d5d-93e0-6fabac1ecaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kg_embeddings(locations, gcn, X_nodes, A_hat_dict):\n",
    "    \"\"\"\n",
    "    locations: list[str], length B\n",
    "    returns: (B, KG_DIM)\n",
    "    \"\"\"\n",
    "\n",
    "    device = X_nodes.device\n",
    "\n",
    "    # 1. Compute KG embedding ONCE per unique location\n",
    "    unique_locations = set(locations)\n",
    "    location_to_embedding = {}\n",
    "\n",
    "    for loc in unique_locations:\n",
    "        A_hat = A_hat_dict[loc]              # (N, N)\n",
    "        kg_emb = gcn(A_hat, X_nodes)         # (KG_DIM,)\n",
    "        location_to_embedding[loc] = kg_emb\n",
    "\n",
    "    # 2. Assign embedding to each sample\n",
    "    kg_embeds = [\n",
    "        location_to_embedding[loc] for loc in locations\n",
    "    ]\n",
    "\n",
    "    return torch.stack(kg_embeds).to(device)   # (B, KG_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f10d89d-2d42-4bd7-9421-b27ec06cb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReportDecoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, fused_feat, input_ids):\n",
    "#         emb = self.embedding(input_ids)      # (B, T, D)\n",
    "#         h0 = fused_feat.unsqueeze(0)          # (1, B, HIDDEN_DIM)\n",
    "#         out, _ = self.gru(emb, h0)\n",
    "#         logits = self.fc(out)                 # (B, T, vocab)\n",
    "#         return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddfd46a5-1f0c-4a92-a8c7-4b1a6167f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = ReportDecoder(\n",
    "#     vocab_size=VOCAB_SIZE,\n",
    "#     embed_dim=EMBED_DIM,\n",
    "#     hidden_dim=HIDDEN_DIM\n",
    "# ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "611f5466-52a2-4b6c-b862-3b3977479d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1a99c02-162d-4c63-8f4d-d1795a84f7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc449b413804bbab014b16e989b8cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "BIOGPT_NAME = \"microsoft/BioGPT-Large\"\n",
    "\n",
    "decoder = AutoModelForCausalLM.from_pretrained(\n",
    "    BIOGPT_NAME\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c1b1b43-40ca-4857-a044-a64d452fa109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioGptScaledWordEmbedding(57727, 1600, padding_idx=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f96a1a39-8816-4d09-aee1-61a100c8214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable decoder parameters: 1\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Freeze BioGPT (CRITICAL)\n",
    "# =========================\n",
    "\n",
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only allow token embeddings to update\n",
    "decoder.get_input_embeddings().weight.requires_grad = True\n",
    "\n",
    "print(\"Trainable decoder parameters:\",\n",
    "      sum(p.requires_grad for p in decoder.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "effbcd29-487a-4ef0-9fde-d73f7f93a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_proj = nn.Linear(\n",
    "    HIDDEN_DIM,\n",
    "    decoder.config.hidden_size\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ed8412b-a828-4a8d-82f4-d69ad7380979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN initialized\n"
     ]
    }
   ],
   "source": [
    "KG_IN_DIM = X_nodes.shape[1]   # number of node features\n",
    "KG_HIDDEN_DIM = 256            # you can tune this\n",
    "KG_OUT_DIM = EMBED_DIM         # must match fusion input\n",
    "\n",
    "gcn = GCN(\n",
    "    in_dim=KG_IN_DIM,\n",
    "    hidden_dim=KG_HIDDEN_DIM,\n",
    "    out_dim=KG_OUT_DIM,\n",
    "    num_layers=2\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"GCN initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee472c5d-d1f5-444a-a0c3-00c8bc3106fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "\n",
    "params = (\n",
    "    list(ct_encoder.parameters()) +\n",
    "    list(mri_encoder.parameters()) +\n",
    "    list(text_encoder.parameters()) +\n",
    "    list(gcn.parameters()) +\n",
    "    list(fusion.parameters()) +\n",
    "    [p for p in decoder.parameters() if p.requires_grad]\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b52a620b-10fb-436e-970f-f0e939a345ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_one_epoch(train_loader):\n",
    "#     ct_encoder.train()\n",
    "#     mri_encoder.train()\n",
    "#     text_encoder.train()\n",
    "#     gcn.train()\n",
    "#     fusion.train()\n",
    "#     decoder.train()\n",
    "\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     pbar = tqdm(\n",
    "#         train_loader,\n",
    "#         desc=\"Training\",\n",
    "#         total=len(train_loader),\n",
    "#         leave=True\n",
    "#     )\n",
    "\n",
    "#     for batch_idx, batch in enumerate(pbar):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # ---- Move tensors to device ----\n",
    "#         ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "#         ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "#         mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "#         mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "#         text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "#         text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "#         locations = batch[\"locations\"]\n",
    "\n",
    "#         # ---- Image encoders ----\n",
    "#         ct_feats = ct_encoder(ct_imgs)\n",
    "#         mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "#         ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "#         mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "#         # ---- Text encoder ----\n",
    "#         text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "#         # ---- KG encoder (location-specific) ----\n",
    "#         kg_feat = get_kg_embeddings(\n",
    "#             locations, gcn, X_nodes, A_hat_dict\n",
    "#         )\n",
    "\n",
    "#         # ---- Fusion ----\n",
    "#         fused_feat = fusion(\n",
    "#             ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "#         )\n",
    "\n",
    "#         # ---- Decoder (teacher forcing) ----\n",
    "#         logits = decoder(\n",
    "#             fused_feat,\n",
    "#             report_ids[:, :-1]\n",
    "#         )\n",
    "\n",
    "#         targets = report_ids[:, 1:]\n",
    "\n",
    "#         loss = criterion(\n",
    "#             logits.reshape(-1, VOCAB_SIZE),\n",
    "#             targets.reshape(-1)\n",
    "#         )\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # ---- Update progress bar ----\n",
    "#         pbar.set_postfix(\n",
    "#             loss=f\"{loss.item():.4f}\"\n",
    "#         )\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(train_loader):\n",
    "    ct_encoder.train()\n",
    "    mri_encoder.train()\n",
    "    text_encoder.train()\n",
    "    gcn.train()\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "    prefix_proj.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Training\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ---- Move tensors to device ----\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        report_mask = batch[\"report_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # ---- Image encoders ----\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        # ---- Text encoder ----\n",
    "        text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        # ---- KG encoder ----\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        # ---- Fusion ----\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )  # (B, EMBED_DIM)\n",
    "\n",
    "        # ======================================================\n",
    "        # 🔑 BioGPT decoder with PREFIX CONDITIONING (CORRECT)\n",
    "        # ======================================================\n",
    "        \n",
    "        # ---- Project fused features to prefix ----\n",
    "        prefix = prefix_proj(fused_feat).unsqueeze(1)  # (B, 1, H)\n",
    "\n",
    "        # ======================================================\n",
    "        # 🔑 Inject location-specific BOS tokens (MANUAL)\n",
    "        # ======================================================\n",
    "        \n",
    "        B = report_ids.size(0)\n",
    "        \n",
    "        # Convert location → BOS token id\n",
    "        bos_ids = [\n",
    "            tokenizer.convert_tokens_to_ids(\n",
    "                LOCATION_TOKENS[loc][\"bos\"]\n",
    "            )\n",
    "            for loc in locations\n",
    "        ]\n",
    "        \n",
    "        bos_ids = torch.tensor(\n",
    "            bos_ids,\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)   # (B, 1)\n",
    "        \n",
    "        # Prepend BOS to report_ids\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "        \n",
    "        # Update report mask\n",
    "        bos_mask = torch.ones(\n",
    "            (B, 1),\n",
    "            device=report_mask.device\n",
    "        )\n",
    "        report_mask = torch.cat([bos_mask, report_mask], dim=1)\n",
    "        \n",
    "\n",
    "        # ---- Token embeddings ----\n",
    "        token_embeds = decoder.get_input_embeddings()(report_ids)\n",
    "        inputs_embeds = torch.cat([prefix, token_embeds], dim=1)\n",
    "        \n",
    "        # ---- Attention mask (add prefix mask) ----\n",
    "        prefix_mask = torch.ones(\n",
    "            (report_mask.size(0), 1),\n",
    "            device=report_mask.device\n",
    "        )\n",
    "        attention_mask = torch.cat([prefix_mask, report_mask], dim=1)\n",
    "        \n",
    "        # ======================================================\n",
    "        # 🔥 CORRECT LABEL SHIFT (THIS IS THE KEY FIX)\n",
    "        # ======================================================\n",
    "        \n",
    "        # Clone report ids\n",
    "        labels = report_ids.clone()\n",
    "        \n",
    "        # Ignore padding tokens\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Add dummy label for prefix position\n",
    "        prefix_labels = torch.full(\n",
    "            (labels.size(0), 1),\n",
    "            -100,\n",
    "            device=labels.device\n",
    "        )\n",
    "        \n",
    "        # Final labels align with inputs_embeds\n",
    "        labels = torch.cat([prefix_labels, labels], dim=1)\n",
    "        \n",
    "        # ---- BioGPT forward ----\n",
    "        outputs = decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac49ee64-8510-435a-ae10-edee63eb6586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57727 57727\n",
      "<HEAD_BOS> 57717\n",
      "torch.Size([1600])\n"
     ]
    }
   ],
   "source": [
    "# 1. Same vocab size\n",
    "print(len(tokenizer), decoder.get_input_embeddings().weight.shape[0])\n",
    "\n",
    "# 2. Token → ID consistency\n",
    "tok = \"<HEAD_BOS>\"\n",
    "tid = tokenizer.convert_tokens_to_ids(tok)\n",
    "print(tok, tid)\n",
    "\n",
    "# 3. Embedding lookup works\n",
    "emb = decoder.get_input_embeddings().weight[tid]\n",
    "print(emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c2f4935-6cd1-4dec-ab73-b2fabdb29622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                                                                                | 0/160 [00:00<?, ?it/s]\u001b[AXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709898e8af2240b4981ac2cac61a6eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                                                                   | 0/160 [01:21<?, ?it/s, loss=3.6104]\u001b[A\n",
      "Training:   1%|▎                                                        | 1/160 [01:21<3:35:55, 81.48s/it, loss=3.6104]\u001b[A\n",
      "Training:   1%|▎                                                        | 1/160 [01:51<3:35:55, 81.48s/it, loss=4.1637]\u001b[A\n",
      "Training:   1%|▋                                                        | 2/160 [01:51<2:14:43, 51.16s/it, loss=4.1637]\u001b[A\n",
      "Training:   1%|▋                                                        | 2/160 [02:08<2:14:43, 51.16s/it, loss=4.0992]\u001b[A\n",
      "Training:   2%|█                                                        | 3/160 [02:08<1:33:38, 35.79s/it, loss=4.0992]\u001b[A\n",
      "Training:   2%|█                                                        | 3/160 [02:24<1:33:38, 35.79s/it, loss=3.9414]\u001b[A\n",
      "Training:   2%|█▍                                                       | 4/160 [02:24<1:12:32, 27.90s/it, loss=3.9414]\u001b[A\n",
      "Training:   2%|█▍                                                       | 4/160 [02:51<1:12:32, 27.90s/it, loss=3.4986]\u001b[A\n",
      "Training:   3%|█▊                                                       | 5/160 [02:51<1:10:43, 27.38s/it, loss=3.4986]\u001b[A\n",
      "Training:   3%|█▊                                                       | 5/160 [03:26<1:10:43, 27.38s/it, loss=3.8931]\u001b[A\n",
      "Training:   4%|██▏                                                      | 6/160 [03:26<1:16:52, 29.95s/it, loss=3.8931]\u001b[A\n",
      "Training:   4%|██▏                                                      | 6/160 [03:55<1:16:52, 29.95s/it, loss=3.5610]\u001b[A\n",
      "Training:   4%|██▍                                                      | 7/160 [04:20<1:34:53, 37.21s/it, loss=3.5610]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m NUM_EPOCHS = \u001b[32m5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m      6\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(train_loader)\u001b[39m\n\u001b[32m    212\u001b[39m outputs = decoder(\n\u001b[32m    213\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    214\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    215\u001b[39m     labels=labels\n\u001b[32m    216\u001b[39m )\n\u001b[32m    218\u001b[39m loss = outputs.loss\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m optimizer.step()\n\u001b[32m    222\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\fyp_manish_shyam_phase2\\my_env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\fyp_manish_shyam_phase2\\my_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\fyp_manish_shyam_phase2\\my_env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5ce2e-48a8-451e-9d20-8f181e02d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(fused_feat, location, max_len=MAX_REPORT_LEN):\n",
    "#     \"\"\"\n",
    "#     fused_feat: (1, EMBED_DIM)\n",
    "#     location: str\n",
    "#     returns: generated report (str)\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder.eval()\n",
    "\n",
    "#     bos_token = LOCATION_TOKENS[location][\"bos\"]\n",
    "#     eos_token = LOCATION_TOKENS[location][\"eos\"]\n",
    "\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(bos_token)\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "\n",
    "#     for _ in range(max_len):\n",
    "#         inp = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)   # (1, T)\n",
    "\n",
    "#         logits = decoder(fused_feat, inp)      # (1, T, vocab)\n",
    "#         next_id = logits[0, -1].argmax(dim=-1).item()\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "\n",
    "#         if next_id == eos_iC:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(fused_feat, location, max_len=MAX_REPORT_LEN):\n",
    "#     decoder.eval()\n",
    "#     prefix_proj.eval()\n",
    "\n",
    "#     # ---- Prefix ----\n",
    "#     prefix = prefix_proj(fused_feat).unsqueeze(1)  # (1, 1, H)\n",
    "\n",
    "#     # ---- BOS token ----\n",
    "#     bos_token = LOCATION_TOKENS[location][\"bos\"]\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(bos_token)\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "\n",
    "#     for _ in range(max_len):\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)  # (1, T)\n",
    "\n",
    "#         token_embeds = decoder.get_input_embeddings()(input_ids)\n",
    "\n",
    "#         inputs_embeds = torch.cat(\n",
    "#             [prefix, token_embeds],\n",
    "#             dim=1\n",
    "#         )\n",
    "\n",
    "#         attention_mask = torch.ones(\n",
    "#             inputs_embeds.size()[:-1],\n",
    "#             device=DEVICE\n",
    "#         )\n",
    "\n",
    "#         outputs = decoder(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "\n",
    "#         next_token_logits = outputs.logits[0, -1]\n",
    "#         next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "\n",
    "#         if next_id == tokenizer.convert_tokens_to_ids(\n",
    "#             LOCATION_TOKENS[location][\"eos\"]\n",
    "#         ):\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(\n",
    "#     fused_feat,\n",
    "#     location,\n",
    "#     max_len=MAX_REPORT_LEN,\n",
    "#     min_len=20\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     fused_feat: (1, EMBED_DIM)\n",
    "#     location: str\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder.eval()\n",
    "#     prefix_proj.eval()\n",
    "\n",
    "#     # ---- Prefix conditioning ----\n",
    "#     prefix = prefix_proj(fused_feat).unsqueeze(1)  # (1, 1, H)\n",
    "\n",
    "#     # ---- Location-specific tokens ----\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"bos\"]\n",
    "#     )\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"eos\"]\n",
    "#     )\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "#     word_count = 0   # counts generated tokens EXCLUDING BOS\n",
    "\n",
    "#     while word_count < max_len:\n",
    "\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)  # (1, T)\n",
    "\n",
    "#         # ---- Token embeddings ----\n",
    "#         token_embeds = decoder.get_input_embeddings()(input_ids)\n",
    "\n",
    "#         # ---- Prefix + tokens ----\n",
    "#         inputs_embeds = torch.cat(\n",
    "#             [prefix, token_embeds],\n",
    "#             dim=1\n",
    "#         )  # (1, 1+T, H)\n",
    "\n",
    "#         # ---- Attention mask ----\n",
    "#         attention_mask = torch.ones(\n",
    "#             inputs_embeds.size()[:2],\n",
    "#             device=DEVICE\n",
    "#         )\n",
    "\n",
    "#         outputs = decoder(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "\n",
    "#         next_token_logits = outputs.logits[0, -1]\n",
    "#         next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "#         # --------------------------------------------------\n",
    "#         # 🚫 Ignore EOS if min_len not reached\n",
    "#         # --------------------------------------------------\n",
    "#         if next_id == eos_id and word_count < min_len:\n",
    "#             continue\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "\n",
    "#         # Count only real tokens (not BOS, not ignored EOS)\n",
    "#         word_count += 1\n",
    "\n",
    "#         # ---- Stop only if EOS AFTER min_len ----\n",
    "#         if next_id == eos_iC:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_report(\n",
    "    fused_feat,\n",
    "    location,\n",
    "    max_len=MAX_REPORT_LEN,\n",
    "    min_len=20,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    fused_feat: (1, EMBED_DIM)\n",
    "    location: str\n",
    "    \"\"\"\n",
    "\n",
    "    decoder.eval()\n",
    "    prefix_proj.eval()\n",
    "\n",
    "    # ---- Prefix conditioning ----\n",
    "    prefix = prefix_proj(fused_feat).unsqueeze(1)  # (1, 1, H)\n",
    "\n",
    "    # ---- Location-specific tokens ----\n",
    "    bos_id = tokenizer.convert_tokens_to_ids(\n",
    "        LOCATION_TOKENS[location][\"bos\"]\n",
    "    )\n",
    "    eos_id = tokenizer.convert_tokens_to_ids(\n",
    "        LOCATION_TOKENS[location][\"eos\"]\n",
    "    )\n",
    "\n",
    "    generated_ids = [bos_id]\n",
    "    word_count = 0\n",
    "\n",
    "    for _ in range(max_len * 2):  # safety cap\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            generated_ids,\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        token_embeds = decoder.get_input_embeddings()(input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat(\n",
    "            [prefix, token_embeds],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        attention_mask = torch.ones(\n",
    "            inputs_embeds.size()[:2],\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        outputs = decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[0, -1] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # ==================================================\n",
    "        # 🔑 TOP-P (NUCLEUS) SAMPLING\n",
    "        # ==================================================\n",
    "        sorted_probs, sorted_indices = torch.sort(\n",
    "            probs, descending=True\n",
    "        )\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "\n",
    "        # Keep smallest set whose cumulative prob >= top_p\n",
    "        cutoff = cumulative_probs > top_p\n",
    "        cutoff[1:] = cutoff[:-1].clone()\n",
    "        cutoff[0] = False\n",
    "\n",
    "        sorted_probs[cutoff] = 0.0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "        next_id = torch.multinomial(sorted_probs, 1).item()\n",
    "        next_id = sorted_indices[next_id].item()\n",
    "\n",
    "        # ---- Ignore early EOS ----\n",
    "        if next_id == eos_id and word_count < min_len:\n",
    "            continue\n",
    "\n",
    "        generated_ids.append(next_id)\n",
    "        word_count += 1\n",
    "\n",
    "        if next_id == eos_iC:\n",
    "            break\n",
    "\n",
    "        if word_count >= max_len:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff3156-f2d5-4174-9355-c599eb2349e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def run_inference(test_loader):\n",
    "#     ct_encoder.eval()\n",
    "#     mri_encoder.eval()\n",
    "#     text_encoder.eval()\n",
    "#     gcn.eval()\n",
    "#     fusion.eval()\n",
    "#     decoder.eval()\n",
    "\n",
    "#     generated_reports = []\n",
    "#     ground_truth_reports = []\n",
    "#     locations_all = []\n",
    "\n",
    "#     pbar = tqdm(\n",
    "#         test_loader,\n",
    "#         desc=\"Generating reports\",\n",
    "#         total=len(test_loader),\n",
    "#         leave=True\n",
    "#     )\n",
    "\n",
    "#     for batch in pbar:\n",
    "#         # ---- Move tensors ----\n",
    "#         ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "#         ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "#         mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "#         mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "#         text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "#         text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         report_ids = batch[\"report_input_ids\"]   # keep on CPU for decoding GT\n",
    "#         locations = batch[\"locations\"]\n",
    "\n",
    "#         # ---- Encode images ----\n",
    "#         ct_feats = ct_encoder(ct_imgs)\n",
    "#         mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "#         ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "#         mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "#         # ---- Encode text ----\n",
    "#         text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "#         # ---- KG embeddings ----\n",
    "#         kg_feat = get_kg_embeddings(\n",
    "#             locations, gcn, X_nodes, A_hat_dict\n",
    "#         )\n",
    "\n",
    "#         # ---- Fusion ----\n",
    "#         fused_feats = fusion(\n",
    "#             ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "#         )   # (B, EMBED_DIM)\n",
    "\n",
    "#         # ---- Generate per sample ----\n",
    "#         B = fused_feats.size(0)\n",
    "\n",
    "#         for i in range(B):\n",
    "#             gen_report = generate_report(\n",
    "#                 fused_feats[i].unsqueeze(0),\n",
    "#                 locations[i]\n",
    "#             )\n",
    "\n",
    "#             gt_report = tokenizer.decode(\n",
    "#                 report_ids[i],\n",
    "#                 skip_special_tokens=True\n",
    "#             )\n",
    "\n",
    "#             generated_reports.append(gen_report)\n",
    "#             ground_truth_reports.append(gt_report)\n",
    "#             locations_all.append(locations[i])\n",
    "\n",
    "#     return generated_reports, ground_truth_reports, locations_all\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(test_loader):\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "    fusion.eval()\n",
    "    decoder.eval()\n",
    "    prefix_proj.eval()   # 🔑 FIX 1\n",
    "\n",
    "    generated_reports = []\n",
    "    ground_truth_reports = []\n",
    "    locations_all = []\n",
    "\n",
    "    pbar = tqdm(\n",
    "        test_loader,\n",
    "        desc=\"Generating reports\",\n",
    "        total=len(test_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        # ---- Move tensors ----\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"]  # CPU OK\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # ---- Encode images ----\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        # ---- Encode text ----\n",
    "        text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        # ---- KG embeddings ----\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        # ---- Fusion ----\n",
    "        fused_feats = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )   # (B, EMBED_DIM)\n",
    "\n",
    "        # ---- Generate per sample ----\n",
    "        for i in range(fused_feats.size(0)):\n",
    "\n",
    "            gen_report = generate_report(\n",
    "                fused_feats[i].unsqueeze(0),\n",
    "                locations[i]\n",
    "            )\n",
    "\n",
    "            # 🔑 FIX 2: align GT with training format\n",
    "            bos_token = LOCATION_TOKENS[locations[i]][\"bos\"]\n",
    "            gt_report = bos_token + \" \" + tokenizer.decode(\n",
    "                report_ids[i],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            generated_reports.append(gen_report)\n",
    "            ground_truth_reports.append(gt_report)\n",
    "            locations_all.append(locations[i])\n",
    "\n",
    "    return generated_reports, ground_truth_reports, locations_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f64d93-576a-469e-afb2-514c63f9c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a24711-5da5-482e-bceb-d6b1fa89ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afed621-58d0-44aa-8620-5df2c6253752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

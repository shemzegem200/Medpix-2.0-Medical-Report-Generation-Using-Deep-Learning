{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b88127-810f-44d1-ae5e-8c55f542db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ad9c51-ff2c-4eec-8a15-fce9dae20d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.1+cu126\n",
      "torchvision: 0.22.1+cu126\n",
      "âœ… torchvision fully loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "print(\"âœ… torchvision fully loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf99192-6b64-436d-a50d-3ed9f5038408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b6c44f-c0c7-4e24-b881-5418ab7e4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_REPORT_LEN = 120\n",
    "EMBED_DIM = 256\n",
    "TEXT_DIM = 768\n",
    "KG_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "# NUM_WORKERS = 4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa24167f-d5e8-4528-8a88-4aaa0fbe550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facda5f1-6fee-499a-99ab-fb18423a2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2fc8218-9260-411b-a11d-91c86cc8fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_TOKENS = {\n",
    "    \"Head\": {\n",
    "        \"bos\": \"<HEAD_BOS>\",\n",
    "        \"eos\": \"<HEAD_EOS>\"\n",
    "    },\n",
    "    \"Thorax\": {\n",
    "        \"bos\": \"<THORAX_BOS>\",\n",
    "        \"eos\": \"<THORAX_EOS>\"\n",
    "    },\n",
    "    \"Abdomen\": {\n",
    "        \"bos\": \"<ABDOMEN_BOS>\",\n",
    "        \"eos\": \"<ABDOMEN_EOS>\"\n",
    "    },\n",
    "    \"Spine and Muscles\": {\n",
    "        \"bos\": \"<SPINE_BOS>\",\n",
    "        \"eos\": \"<SPINE_EOS>\"\n",
    "    },\n",
    "    \"Reproductive and Urinary System\": {\n",
    "        \"bos\": \"<GU_BOS>\",\n",
    "        \"eos\": \"<GU_EOS>\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98215c67-208f-4dc4-84c4-31d3f4d4909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30533\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "special_tokens = {\n",
    "    \"pad_token\": \"<PAD>\",\n",
    "    \"additional_special_tokens\": []\n",
    "}\n",
    "\n",
    "for loc in LOCATION_TOKENS:\n",
    "    special_tokens[\"additional_special_tokens\"].append(\n",
    "        LOCATION_TOKENS[loc][\"bos\"]\n",
    "    )\n",
    "    special_tokens[\"additional_special_tokens\"].append(\n",
    "        LOCATION_TOKENS[loc][\"eos\"]\n",
    "    )\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394cbe35-d47e-48ff-a4b8-e3fb761c3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MedPixDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Normalize NaNs early (VERY IMPORTANT)\n",
    "        self.df = self.df.fillna(\"\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def load_image(self, img_path):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def parse_image_list(self, s):\n",
    "        # CSV stores lists as strings: \"['path1', 'path2']\"\n",
    "        if s == \"\" or s == \"[]\":\n",
    "            return []\n",
    "        return ast.literal_eval(s)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # -------- Images --------\n",
    "        ct_images = []\n",
    "        mri_images = []\n",
    "\n",
    "        ct_paths = self.parse_image_list(row[\"CT_image_paths\"])\n",
    "        mri_paths = self.parse_image_list(row[\"MRI_image_paths\"])\n",
    "\n",
    "        for p in ct_paths:\n",
    "            ct_images.append(self.load_image(p))\n",
    "\n",
    "        for p in mri_paths:\n",
    "            mri_images.append(self.load_image(p))\n",
    "\n",
    "        # -------- Text Encoder Input --------\n",
    "        text_input = row[\"combined_text\"]\n",
    "\n",
    "        # -------- Target Report --------\n",
    "        report = row[\"findings\"]\n",
    "\n",
    "        # -------- Location (KG routing) --------\n",
    "        location_category = row[\"Location Category\"]\n",
    "\n",
    "        return {\n",
    "            \"uid\": row[\"U_id\"],\n",
    "            \"ct_images\": ct_images,     # list[Tensor]\n",
    "            \"mri_images\": mri_images,   # list[Tensor]\n",
    "            \"text_input\": text_input,   # str\n",
    "            \"report\": report,           # str\n",
    "            \"location\": location_category\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252ca940-92c8-4b9c-8494-74ab73f4afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch items contain:\n",
    "    - ct_images: list[Tensor]\n",
    "    - mri_images: list[Tensor]\n",
    "    - text_input: str\n",
    "    - report: str\n",
    "    - location: str\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- CT images ----------\n",
    "    max_ct = max(len(b[\"ct_images\"]) for b in batch)\n",
    "    ct_imgs, ct_masks = [], []\n",
    "\n",
    "    for b in batch:\n",
    "        imgs = b[\"ct_images\"]\n",
    "        if len(imgs) == 0:\n",
    "            dummy = torch.zeros(3, 224, 224)\n",
    "            imgs = [dummy]\n",
    "\n",
    "        pad = max_ct - len(imgs)\n",
    "        imgs = imgs + [torch.zeros_like(imgs[0])] * pad\n",
    "        mask = [1] * (len(imgs) - pad) + [0] * pad\n",
    "\n",
    "        ct_imgs.append(torch.stack(imgs))\n",
    "        ct_masks.append(torch.tensor(mask))\n",
    "\n",
    "    ct_imgs = torch.stack(ct_imgs)      # (B, N_ct, 3, H, W)\n",
    "    ct_masks = torch.stack(ct_masks)    # (B, N_ct)\n",
    "\n",
    "    # ---------- MRI images ----------\n",
    "    max_mri = max(len(b[\"mri_images\"]) for b in batch)\n",
    "    mri_imgs, mri_masks = [], []\n",
    "\n",
    "    for b in batch:\n",
    "        imgs = b[\"mri_images\"]\n",
    "        if len(imgs) == 0:\n",
    "            dummy = torch.zeros(3, 224, 224)\n",
    "            imgs = [dummy]\n",
    "\n",
    "        pad = max_mri - len(imgs)\n",
    "        imgs = imgs + [torch.zeros_like(imgs[0])] * pad\n",
    "        mask = [1] * (len(imgs) - pad) + [0] * pad\n",
    "\n",
    "        mri_imgs.append(torch.stack(imgs))\n",
    "        mri_masks.append(torch.tensor(mask))\n",
    "\n",
    "    mri_imgs = torch.stack(mri_imgs)    # (B, N_mri, 3, H, W)\n",
    "    mri_masks = torch.stack(mri_masks)  # (B, N_mri)\n",
    "\n",
    "    # ---------- Text encoder input ----------\n",
    "    text_inputs = [b[\"text_input\"] for b in batch]\n",
    "    text_enc = tokenizer(\n",
    "        text_inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------- Decoder target ----------\n",
    "    reports = [b[\"report\"] for b in batch]   # <-- RAW ground truth\n",
    "    report_enc = tokenizer(\n",
    "        reports,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_REPORT_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ---------- Location ----------\n",
    "    locations = [b[\"location\"] for b in batch]\n",
    "\n",
    "    return {\n",
    "        \"ct_images\": ct_imgs,\n",
    "        \"ct_masks\": ct_masks,\n",
    "        \"mri_images\": mri_imgs,\n",
    "        \"mri_masks\": mri_masks,\n",
    "        \"text_input_ids\": text_enc[\"input_ids\"],\n",
    "        \"text_attention_mask\": text_enc[\"attention_mask\"],\n",
    "        \"report_input_ids\": report_enc[\"input_ids\"],\n",
    "        \"report_attention_mask\": report_enc[\"attention_mask\"],\n",
    "        \"reports\": reports,              # <-- RAW ground-truth strings\n",
    "        \"locations\": locations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6311cd02-d41d-48b2-ab18-b72fe0451f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPX1009\n",
      "CT images: 2\n",
      "MRI images: 0\n",
      "Text length: 379\n",
      "Report length: 152\n",
      "Location: Reproductive and Urinary System\n"
     ]
    }
   ],
   "source": [
    "ds = MedPixDataset(r\"C:\\fyp_manish_shyam_phase2\\data\\df_overall.csv\", transform=image_transform)\n",
    "\n",
    "sample = ds[0]\n",
    "print(sample[\"uid\"])\n",
    "print(\"CT images:\", len(sample[\"ct_images\"]))\n",
    "print(\"MRI images:\", len(sample[\"mri_images\"]))\n",
    "print(\"Text length:\", len(sample[\"text_input\"]))\n",
    "print(\"Report length:\", len(sample[\"report\"]))\n",
    "print(\"Location:\", sample[\"location\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "949bc98e-c4fe-4b56-ab60-b52f9a9017f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda71f90-3260-4a5e-b38b-1a9e4773b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT images shape: torch.Size([3, 2, 3, 224, 224])\n",
      "CT masks shape: torch.Size([3, 2])\n",
      "MRI images shape: torch.Size([3, 1, 3, 224, 224])\n",
      "MRI masks shape: torch.Size([3, 2])\n",
      "Text input ids shape: torch.Size([3, 90])\n",
      "Report input ids shape: torch.Size([3, 73])\n",
      "Raw reports count: 3\n",
      "First report preview:\n",
      " Bladder with thickened wall and diverticulum on the right. Diverticulum is mostly likely secondary to chronic outflow obstruction. Prostate enlargement.\n",
      "Locations: ['Reproductive and Urinary System', 'Thorax', 'Reproductive and Urinary System']\n"
     ]
    }
   ],
   "source": [
    "# Taking 2â€“3 samples manually for doing a small sanity check\n",
    "batch_samples = [ds[i] for i in range(3)]\n",
    "\n",
    "batch = collate_fn(batch_samples)\n",
    "print(\"CT images shape:\", batch[\"ct_images\"].shape)\n",
    "print(\"CT masks shape:\", batch[\"ct_masks\"].shape)\n",
    "\n",
    "print(\"MRI images shape:\", batch[\"mri_images\"].shape)\n",
    "print(\"MRI masks shape:\", batch[\"mri_masks\"].shape)\n",
    "\n",
    "print(\"Text input ids shape:\", batch[\"text_input_ids\"].shape)\n",
    "print(\"Report input ids shape:\", batch[\"report_input_ids\"].shape)\n",
    "\n",
    "print(\"Raw reports count:\", len(batch[\"reports\"]))\n",
    "print(\"First report preview:\\n\", batch[\"reports\"][0][:200])\n",
    "\n",
    "print(\"Locations:\", batch[\"locations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0001bd89-f0fe-4dd5-b337-c529ddfd423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 671\n",
      "Train samples: 637\n",
      "Test samples:  34\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# ---- Load full dataset ----\n",
    "full_dataset = MedPixDataset(\n",
    "    r\"C:\\fyp_manish_shyam_phase2\\data\\df_overall.csv\",\n",
    "    transform=image_transform\n",
    ")\n",
    "\n",
    "# ---- 80 / 20 split ----\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.95 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {dataset_size}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples:  {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b688522-5b96-4c03-b666-61dbafe7520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train & Test loaders ready\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn\n",
    "    # , pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,          # No shuffle for test\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn\n",
    "    # , pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Train & Test loaders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573ecc90-ab9e-40d2-b629-720770eeeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        for p in base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        base.fc = nn.Linear(base.fc.in_features, embed_dim)\n",
    "        self.cnn = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, 3, H, W)\n",
    "        return: (B, N, D)\n",
    "        \"\"\"\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "        feats = self.cnn(x)\n",
    "        feats = feats.view(B, N, -1)\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f6df810-7bae-41ac-92c1-78f1c5838f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_encoder = ImageEncoder().to(DEVICE)\n",
    "mri_encoder = ImageEncoder().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "444b5b38-d67b-4b11-9f60-49ab57ce0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_mean_pooling(feats, masks):\n",
    "    masks = masks.unsqueeze(-1).float()   # (B, N, 1)\n",
    "    summed = (feats * masks).sum(dim=1)\n",
    "    denom = masks.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4ae7796-84ff-40a8-b22d-058f6ec9d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "\n",
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.proj = nn.Linear(TEXT_DIM, EMBED_DIM)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         out = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         cls = out.last_hidden_state[:, 0]  # CLS token\n",
    "#         return self.proj(cls)\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, embed_dim):\n",
    "        super().__init__()\n",
    "        self.lm = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        hidden_dim = self.lm.config.hidden_size\n",
    "        self.proj = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Mean pooling over tokens (causal models do NOT have CLS)\n",
    "        hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "\n",
    "        pooled = (hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        return self.proj(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6270bce5-2b14-4bc7-8433-d5ef8864ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30533, 768, padding_idx=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "text_encoder = TextEncoder(\n",
    "    model_name=TEXT_MODEL_NAME,\n",
    "    embed_dim=EMBED_DIM\n",
    ").to(DEVICE)\n",
    "\n",
    "text_encoder.lm.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f58656a4-447a-43af-9d8d-42dbf8ed7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, A_hat, X):\n",
    "        \"\"\"\n",
    "        A_hat: (N, N) normalized adjacency\n",
    "        X: (N, D)\n",
    "        \"\"\"\n",
    "        return F.relu(self.linear(A_hat @ X))\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(GCNLayer(dims[i], dims[i + 1]))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, A_hat, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(A_hat, X)\n",
    "        return X.mean(dim=0)   # graph-level embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1794a05c-21ea-42e7-9cbd-2384aebb02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A: (N, N) raw adjacency matrix\n",
    "    returns: (N, N) normalized adjacency with self-loops\n",
    "    \"\"\"\n",
    "    device = A.device\n",
    "    N = A.size(0)\n",
    "\n",
    "    # Add self-loops\n",
    "    A_tilde = A + torch.eye(N, device=device)\n",
    "\n",
    "    # Degree\n",
    "    D = A_tilde.sum(dim=1)\n",
    "\n",
    "    # D^{-1/2}\n",
    "    D_inv_sqrt = torch.pow(D, -0.5)\n",
    "    D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = torch.diag(D_inv_sqrt)\n",
    "\n",
    "    # Symmetric normalization\n",
    "    A_hat = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "    return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "555d53c6-1166-4d3c-8a54-3df126fc4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head: A_hat shape = torch.Size([4400, 4400])\n",
      "Thorax: A_hat shape = torch.Size([4400, 4400])\n",
      "Abdomen: A_hat shape = torch.Size([4400, 4400])\n",
      "Spine and Muscles: A_hat shape = torch.Size([4400, 4400])\n",
      "Reproductive and Urinary System: A_hat shape = torch.Size([4400, 4400])\n",
      "Shared X_nodes shape: torch.Size([4400, 4400])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "KG_LOCATION_MAP = {\n",
    "    \"Head\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Head_matrix.csv\",\n",
    "    \"Thorax\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Thorax_matrix.csv\",\n",
    "    \"Abdomen\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Abdomen_matrix.csv\",\n",
    "    \"Spine and Muscles\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Spine_and_Muscles_matrix.csv\",\n",
    "    \"Reproductive and Urinary System\": r\"C:\\fyp_manish_shyam_phase2\\data\\split_by_location_category_matrices\\Reproductive_and_Urinary_System_matrix.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "A_hat_dict = {}\n",
    "\n",
    "# ---- Load and normalize adjacency matrices ----\n",
    "for loc, path in KG_LOCATION_MAP.items():\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "    A = torch.tensor(\n",
    "        df.values,\n",
    "        dtype=torch.float32,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    A_hat = normalize_adjacency(A)\n",
    "    A_hat_dict[loc] = A_hat\n",
    "\n",
    "    print(f\"{loc}: A_hat shape = {A_hat.shape}\")\n",
    "\n",
    "# ---- Create shared X_nodes (identity) ----\n",
    "# Node count inferred from any adjacency matrix\n",
    "example_loc = next(iter(A_hat_dict))\n",
    "N_nodes = A_hat_dict[example_loc].shape[0]\n",
    "\n",
    "X_nodes = torch.eye(N_nodes, device=DEVICE)\n",
    "\n",
    "print(\"Shared X_nodes shape:\", X_nodes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "683603fc-db9c-4183-b56e-bfd3b0c5171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embed_dim * 4, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, ct_feat, mri_feat, text_feat, kg_feat):\n",
    "        \"\"\"\n",
    "        All inputs: (B, EMBED_DIM)\n",
    "        Output: (B, HIDDEN_DIM)\n",
    "        \"\"\"\n",
    "        fused = torch.cat(\n",
    "            [ct_feat, mri_feat, text_feat, kg_feat],\n",
    "            dim=-1\n",
    "        )\n",
    "        fused = self.dropout(fused)\n",
    "        return self.fc(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da79e065-95d6-4b57-bd4c-5ee9aa682688",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = FeatureFusion().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acbf9208-3be2-4d5d-93e0-6fabac1ecaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kg_embeddings(locations, gcn, X_nodes, A_hat_dict):\n",
    "    \"\"\"\n",
    "    locations: list[str], length B\n",
    "    returns: (B, KG_DIM)\n",
    "    \"\"\"\n",
    "\n",
    "    device = X_nodes.device\n",
    "\n",
    "    # 1. Compute KG embedding ONCE per unique location\n",
    "    unique_locations = set(locations)\n",
    "    location_to_embedding = {}\n",
    "\n",
    "    for loc in unique_locations:\n",
    "        A_hat = A_hat_dict[loc]              # (N, N)\n",
    "        kg_emb = gcn(A_hat, X_nodes)         # (KG_DIM,)\n",
    "        location_to_embedding[loc] = kg_emb\n",
    "\n",
    "    # 2. Assign embedding to each sample\n",
    "    kg_embeds = [\n",
    "        location_to_embedding[loc] for loc in locations\n",
    "    ]\n",
    "\n",
    "    return torch.stack(kg_embeds).to(device)   # (B, KG_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f10d89d-2d42-4bd7-9421-b27ec06cb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportDecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, fused_feat, input_ids):\n",
    "        \"\"\"\n",
    "        fused_feat: (B, HIDDEN_DIM)\n",
    "        input_ids: (B, T)\n",
    "        \"\"\"\n",
    "\n",
    "        emb = self.embedding(input_ids)          # (B, T, D)\n",
    "\n",
    "        h0 = fused_feat.unsqueeze(0)             # (1, B, H)\n",
    "        c0 = torch.zeros_like(h0)                # (1, B, H)\n",
    "\n",
    "        out, _ = self.lstm(emb, (h0, c0))\n",
    "        logits = self.fc(out)                    # (B, T, vocab)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddfd46a5-1f0c-4a92-a8c7-4b1a6167f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ReportDecoderLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ed8412b-a828-4a8d-82f4-d69ad7380979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN initialized\n"
     ]
    }
   ],
   "source": [
    "KG_IN_DIM = X_nodes.shape[1]   # number of node features\n",
    "KG_HIDDEN_DIM = 256            # you can tune this\n",
    "KG_OUT_DIM = EMBED_DIM         # must match fusion input\n",
    "\n",
    "gcn = GCN(\n",
    "    in_dim=KG_IN_DIM,\n",
    "    hidden_dim=KG_HIDDEN_DIM,\n",
    "    out_dim=KG_OUT_DIM,\n",
    "    num_layers=2\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"GCN initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c956511-33ed-48b0-bd79-b53ff7667f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49f40052-4425-4983-8b0f-596134f2a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all encoders and fusion\n",
    "freeze_module(ct_encoder)\n",
    "freeze_module(mri_encoder)\n",
    "freeze_module(text_encoder)\n",
    "freeze_module(gcn)\n",
    "freeze_module(fusion)\n",
    "\n",
    "# Ensure decoder is trainable\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee472c5d-d1f5-444a-a0c3-00c8bc3106fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "params = [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19846779-60db-4b59-b9ac-9730ea6eda58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT Encoder: 0 trainable params\n",
      "MRI Encoder: 0 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 0 trainable params\n",
      "Fusion: 0 trainable params\n",
      "Decoder: 25,056,837 trainable params\n"
     ]
    }
   ],
   "source": [
    "def count_trainable(name, module):\n",
    "    n = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    print(f\"{name}: {n:,} trainable params\")\n",
    "\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b52a620b-10fb-436e-970f-f0e939a345ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_one_epoch(train_loader):\n",
    "#     ct_encoder.train()\n",
    "#     mri_encoder.train()\n",
    "#     text_encoder.train()\n",
    "#     gcn.train()\n",
    "#     fusion.train()\n",
    "#     decoder.train()\n",
    "\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     pbar = tqdm(\n",
    "#         train_loader,\n",
    "#         desc=\"Training\",\n",
    "#         total=len(train_loader),\n",
    "#         leave=True\n",
    "#     )\n",
    "\n",
    "#     for batch_idx, batch in enumerate(pbar):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # ---- Move tensors to device ----\n",
    "#         ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "#         ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "#         mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "#         mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "#         text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "#         text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "#         locations = batch[\"locations\"]\n",
    "\n",
    "#         # ---- Image encoders ----\n",
    "#         ct_feats = ct_encoder(ct_imgs)\n",
    "#         mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "#         ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "#         mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "#         # ---- Text encoder ----\n",
    "#         text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "#         # ---- KG encoder (location-specific) ----\n",
    "#         kg_feat = get_kg_embeddings(\n",
    "#             locations, gcn, X_nodes, A_hat_dict\n",
    "#         )\n",
    "\n",
    "#         # ---- Fusion ----\n",
    "#         fused_feat = fusion(\n",
    "#             ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "#         )\n",
    "\n",
    "#         # ---- Decoder (teacher forcing) ----\n",
    "#         logits = decoder(\n",
    "#             fused_feat,\n",
    "#             report_ids[:, :-1]\n",
    "#         )\n",
    "\n",
    "#         targets = report_ids[:, 1:]\n",
    "\n",
    "#         loss = criterion(\n",
    "#             logits.reshape(-1, VOCAB_SIZE),\n",
    "#             targets.reshape(-1)\n",
    "#         )\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # ---- Update progress bar ----\n",
    "#         pbar.set_postfix(\n",
    "#             loss=f\"{loss.item():.4f}\"\n",
    "#         )\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_one_epoch(train_loader):\n",
    "#     ct_encoder.train()\n",
    "#     mri_encoder.train()\n",
    "#     text_encoder.train()\n",
    "#     gcn.train()\n",
    "#     fusion.train()\n",
    "#     decoder.train()\n",
    "#     prefix_proj.train()\n",
    "\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     pbar = tqdm(\n",
    "#         train_loader,\n",
    "#         desc=\"Training\",\n",
    "#         total=len(train_loader),\n",
    "#         leave=True\n",
    "#     )\n",
    "\n",
    "#     for batch_idx, batch in enumerate(pbar):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # ---- Move tensors to device ----\n",
    "#         ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "#         ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "#         mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "#         mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "#         text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "#         text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "#         report_mask = batch[\"report_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         locations = batch[\"locations\"]\n",
    "\n",
    "#         # ---- Image encoders ----\n",
    "#         ct_feats = ct_encoder(ct_imgs)\n",
    "#         mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "#         ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "#         mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "#         # ---- Text encoder ----\n",
    "#         text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "#         # ---- KG encoder ----\n",
    "#         kg_feat = get_kg_embeddings(\n",
    "#             locations, gcn, X_nodes, A_hat_dict\n",
    "#         )\n",
    "\n",
    "#         # ---- Fusion ----\n",
    "#         fused_feat = fusion(\n",
    "#             ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "#         )  # (B, EMBED_DIM)\n",
    "\n",
    "#         # ======================================================\n",
    "#         # ðŸ”‘ BioGPT decoder with PREFIX CONDITIONING (CORRECT)\n",
    "#         # ======================================================\n",
    "        \n",
    "#         # ---- Project fused features to prefix ----\n",
    "#         prefix = prefix_proj(fused_feat).unsqueeze(1)  # (B, 1, H)\n",
    "\n",
    "#         # ======================================================\n",
    "#         # ðŸ”‘ Inject location-specific BOS tokens (MANUAL)\n",
    "#         # ======================================================\n",
    "        \n",
    "#         B = report_ids.size(0)\n",
    "        \n",
    "#         # Convert location â†’ BOS token id\n",
    "#         bos_ids = [\n",
    "#             tokenizer.convert_tokens_to_ids(\n",
    "#                 LOCATION_TOKENS[loc][\"bos\"]\n",
    "#             )\n",
    "#             for loc in locations\n",
    "#         ]\n",
    "        \n",
    "#         bos_ids = torch.tensor(\n",
    "#             bos_ids,\n",
    "#             device=report_ids.device\n",
    "#         ).unsqueeze(1)   # (B, 1)\n",
    "        \n",
    "#         # Prepend BOS to report_ids\n",
    "#         report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "        \n",
    "#         # Update report mask\n",
    "#         bos_mask = torch.ones(\n",
    "#             (B, 1),\n",
    "#             device=report_mask.device\n",
    "#         )\n",
    "#         report_mask = torch.cat([bos_mask, report_mask], dim=1)\n",
    "        \n",
    "\n",
    "#         # ---- Token embeddings ----\n",
    "#         token_embeds = decoder.get_input_embeddings()(report_ids)\n",
    "#         inputs_embeds = torch.cat([prefix, token_embeds], dim=1)\n",
    "        \n",
    "#         # ---- Attention mask (add prefix mask) ----\n",
    "#         prefix_mask = torch.ones(\n",
    "#             (report_mask.size(0), 1),\n",
    "#             device=report_mask.device\n",
    "#         )\n",
    "#         attention_mask = torch.cat([prefix_mask, report_mask], dim=1)\n",
    "        \n",
    "#         # ======================================================\n",
    "#         # ðŸ”¥ CORRECT LABEL SHIFT (THIS IS THE KEY FIX)\n",
    "#         # ======================================================\n",
    "        \n",
    "#         # Clone report ids\n",
    "#         labels = report_ids.clone()\n",
    "        \n",
    "#         # Ignore padding tokens\n",
    "#         labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "#         # Add dummy label for prefix position\n",
    "#         prefix_labels = torch.full(\n",
    "#             (labels.size(0), 1),\n",
    "#             -100,\n",
    "#             device=labels.device\n",
    "#         )\n",
    "        \n",
    "#         # Final labels align with inputs_embeds\n",
    "#         labels = torch.cat([prefix_labels, labels], dim=1)\n",
    "        \n",
    "#         # ---- BioGPT forward ----\n",
    "#         outputs = decoder(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask,\n",
    "#             labels=labels\n",
    "#         )\n",
    "        \n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(train_loader):\n",
    "    decoder.train()\n",
    "    \n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "    fusion.eval()\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Training\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )  # (B, HIDDEN_DIM)\n",
    "\n",
    "        # ======================================================\n",
    "        # ðŸ”‘ Inject LOCATION-SPECIFIC BOS (PRESERVED)\n",
    "        # ======================================================\n",
    "        B = report_ids.size(0)\n",
    "\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)  # (B, 1)\n",
    "\n",
    "        # Prepend BOS to reports\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        # ======================================================\n",
    "        # ðŸ”¥ Teacher forcing (CORRECT SHIFT)\n",
    "        # ======================================================\n",
    "        decoder_inputs = report_ids[:, :-1]   # includes BOS\n",
    "        targets = report_ids[:, 1:]           # next-token targets\n",
    "\n",
    "        logits = decoder(\n",
    "            fused_feat,\n",
    "            decoder_inputs\n",
    "        )  # (B, T, vocab)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac49ee64-8510-435a-ae10-edee63eb6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Same vocab size\n",
    "# print(len(tokenizer), decoder.get_input_embeddings().weight.shape[0])\n",
    "\n",
    "# # 2. Token â†’ ID consistency\n",
    "# tok = \"<HEAD_BOS>\"\n",
    "# tid = tokenizer.convert_tokens_to_ids(tok)\n",
    "# print(tok, tid)\n",
    "\n",
    "# # 3. Embedding lookup works\n",
    "# emb = decoder.get_input_embeddings().weight[tid]\n",
    "# print(emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c2f4935-6cd1-4dec-ab73-b2fabdb29622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:38<00:00,  4.10it/s, loss=7.0911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 7.3242 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:40<00:00,  3.99it/s, loss=5.4694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 6.0553 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:40<00:00,  3.91it/s, loss=5.6757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 5.6952 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.79it/s, loss=5.7890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 5.3627 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=5.5584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 5.0685 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.77it/s, loss=3.8542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 4.8133 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=4.3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 4.6872 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=4.2601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 4.5688 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=4.0884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 4.4475 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.85it/s, loss=4.3080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 4.3559 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.85it/s, loss=3.6879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 4.2521 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.87it/s, loss=3.7673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 4.2134 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.89it/s, loss=4.2896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 4.1608 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:38<00:00,  4.15it/s, loss=4.6646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 4.1074 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.86it/s, loss=2.8993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 4.0566 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.84it/s, loss=4.1367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Train Loss: 4.0214 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=4.1812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Train Loss: 3.9983 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.75it/s, loss=3.3737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Train Loss: 3.9710 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=3.9404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Train Loss: 3.9499 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=4.0014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Train Loss: 3.9282 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.82it/s, loss=3.4805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Train Loss: 3.9069 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.79it/s, loss=4.9631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Train Loss: 3.9075 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.72it/s, loss=4.4650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Train Loss: 3.8905 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.89it/s, loss=4.3661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Train Loss: 3.8786 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.86it/s, loss=2.9056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Train Loss: 3.8654 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.88it/s, loss=4.6086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Train Loss: 3.8579 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.78it/s, loss=4.2563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Train Loss: 3.8530 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.72it/s, loss=3.7276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Train Loss: 3.8396 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.72it/s, loss=3.7969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Train Loss: 3.8392 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=3.6612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Train Loss: 3.8265 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.72it/s, loss=3.3933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Train Loss: 3.8280 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.80it/s, loss=4.4537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Train Loss: 3.8286 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.83it/s, loss=4.5389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Train Loss: 3.8284 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.80it/s, loss=3.2769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Train Loss: 3.8089 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=4.0415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Train Loss: 3.8167 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.86it/s, loss=3.6625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Train Loss: 3.8063 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.81it/s, loss=4.3699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | Train Loss: 3.8051 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.82it/s, loss=2.7631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | Train Loss: 3.8169 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.67it/s, loss=3.3070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 | Train Loss: 3.8072 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.70it/s, loss=3.6204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 | Train Loss: 3.8064 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=3.3373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 | Train Loss: 3.7959 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.70it/s, loss=3.2230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 | Train Loss: 3.7959 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.78it/s, loss=4.0948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 | Train Loss: 3.7982 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.78it/s, loss=3.8451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 | Train Loss: 3.7964 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.81it/s, loss=3.5061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 | Train Loss: 3.7999 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.80it/s, loss=3.5544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 | Train Loss: 3.7965 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.86it/s, loss=4.1115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 | Train Loss: 3.7967 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.83it/s, loss=3.4597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 | Train Loss: 3.8056 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.88it/s, loss=2.5939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 | Train Loss: 3.7940 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=3.7776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 | Train Loss: 3.8021 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3e5ce2e-48a8-451e-9d20-8f181e02d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(fused_feat, location, max_len=MAX_REPORT_LEN):\n",
    "#     \"\"\"\n",
    "#     fused_feat: (1, EMBED_DIM)\n",
    "#     location: str\n",
    "#     returns: generated report (str)\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder.eval()\n",
    "\n",
    "#     bos_token = LOCATION_TOKENS[location][\"bos\"]\n",
    "#     eos_token = LOCATION_TOKENS[location][\"eos\"]\n",
    "\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(bos_token)\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "\n",
    "#     for _ in range(max_len):\n",
    "#         inp = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)   # (1, T)\n",
    "\n",
    "#         logits = decoder(fused_feat, inp)      # (1, T, vocab)\n",
    "#         next_id = logits[0, -1].argmax(dim=-1).item()\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "\n",
    "#         if next_id == eos_iC:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(fused_feat, location, max_len=MAX_REPORT_LEN):\n",
    "#     decoder.eval()\n",
    "#     prefix_proj.eval()\n",
    "\n",
    "#     # ---- Prefix ----\n",
    "#     prefix = prefix_proj(fused_feat).unsqueeze(1)  # (1, 1, H)\n",
    "\n",
    "#     # ---- BOS token ----\n",
    "#     bos_token = LOCATION_TOKENS[location][\"bos\"]\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(bos_token)\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "\n",
    "#     for _ in range(max_len):\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)  # (1, T)\n",
    "\n",
    "#         token_embeds = decoder.get_input_embeddings()(input_ids)\n",
    "\n",
    "#         inputs_embeds = torch.cat(\n",
    "#             [prefix, token_embeds],\n",
    "#             dim=1\n",
    "#         )\n",
    "\n",
    "#         attention_mask = torch.ones(\n",
    "#             inputs_embeds.size()[:-1],\n",
    "#             device=DEVICE\n",
    "#         )\n",
    "\n",
    "#         outputs = decoder(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "\n",
    "#         next_token_logits = outputs.logits[0, -1]\n",
    "#         next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "\n",
    "#         if next_id == tokenizer.convert_tokens_to_ids(\n",
    "#             LOCATION_TOKENS[location][\"eos\"]\n",
    "#         ):\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(\n",
    "#     fused_feat,\n",
    "#     location,\n",
    "#     max_len=MAX_REPORT_LEN,\n",
    "#     min_len=20\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     fused_feat: (1, EMBED_DIM)\n",
    "#     location: str\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder.eval()\n",
    "#     prefix_proj.eval()\n",
    "\n",
    "#     # ---- Prefix conditioning ----\n",
    "#     prefix = prefix_proj(fused_feat).unsqueeze(1)  # (1, 1, H)\n",
    "\n",
    "#     # ---- Location-specific tokens ----\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"bos\"]\n",
    "#     )\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"eos\"]\n",
    "#     )\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "#     word_count = 0   # counts generated tokens EXCLUDING BOS\n",
    "\n",
    "#     while word_count < max_len:\n",
    "\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)  # (1, T)\n",
    "\n",
    "#         # ---- Token embeddings ----\n",
    "#         token_embeds = decoder.get_input_embeddings()(input_ids)\n",
    "\n",
    "#         # ---- Prefix + tokens ----\n",
    "#         inputs_embeds = torch.cat(\n",
    "#             [prefix, token_embeds],\n",
    "#             dim=1\n",
    "#         )  # (1, 1+T, H)\n",
    "\n",
    "#         # ---- Attention mask ----\n",
    "#         attention_mask = torch.ones(\n",
    "#             inputs_embeds.size()[:2],\n",
    "#             device=DEVICE\n",
    "#         )\n",
    "\n",
    "#         outputs = decoder(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "\n",
    "#         next_token_logits = outputs.logits[0, -1]\n",
    "#         next_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "#         # --------------------------------------------------\n",
    "#         # ðŸš« Ignore EOS if min_len not reached\n",
    "#         # --------------------------------------------------\n",
    "#         if next_id == eos_id and word_count < min_len:\n",
    "#             continue\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "\n",
    "#         # Count only real tokens (not BOS, not ignored EOS)\n",
    "#         word_count += 1\n",
    "\n",
    "#         # ---- Stop only if EOS AFTER min_len ----\n",
    "#         if next_id == eos_iC:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(\n",
    "#     fused_feat,\n",
    "#     location,\n",
    "#     max_len=MAX_REPORT_LEN,\n",
    "#     min_len=20,\n",
    "#     top_p=0.9,\n",
    "#     temperature=1.0\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     fused_feat: (1, EMBED_DIM)\n",
    "#     location: str\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder.eval()\n",
    "#     prefix_proj.eval()\n",
    "\n",
    "#     # ---- Prefix conditioning ----\n",
    "#     prefix = prefix_proj(fused_feat).unsqueeze(1)  # (1, 1, H)\n",
    "\n",
    "#     # ---- Location-specific tokens ----\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"bos\"]\n",
    "#     )\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"eos\"]\n",
    "#     )\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "#     word_count = 0\n",
    "\n",
    "#     for _ in range(max_len * 2):  # safety cap\n",
    "\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)\n",
    "\n",
    "#         token_embeds = decoder.get_input_embeddings()(input_ids)\n",
    "\n",
    "#         inputs_embeds = torch.cat(\n",
    "#             [prefix, token_embeds],\n",
    "#             dim=1\n",
    "#         )\n",
    "\n",
    "#         attention_mask = torch.ones(\n",
    "#             inputs_embeds.size()[:2],\n",
    "#             device=DEVICE\n",
    "#         )\n",
    "\n",
    "#         outputs = decoder(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "\n",
    "#         logits = outputs.logits[0, -1] / temperature\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#         # ==================================================\n",
    "#         # ðŸ”‘ TOP-P (NUCLEUS) SAMPLING\n",
    "#         # ==================================================\n",
    "#         sorted_probs, sorted_indices = torch.sort(\n",
    "#             probs, descending=True\n",
    "#         )\n",
    "#         cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "\n",
    "#         # Keep smallest set whose cumulative prob >= top_p\n",
    "#         cutoff = cumulative_probs > top_p\n",
    "#         cutoff[1:] = cutoff[:-1].clone()\n",
    "#         cutoff[0] = False\n",
    "\n",
    "#         sorted_probs[cutoff] = 0.0\n",
    "#         sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "#         next_id = torch.multinomial(sorted_probs, 1).item()\n",
    "#         next_id = sorted_indices[next_id].item()\n",
    "\n",
    "#         # ---- Ignore early EOS ----\n",
    "#         if next_id == eos_id and word_count < min_len:\n",
    "#             continue\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "#         word_count += 1\n",
    "\n",
    "#         if next_id == eos_iC:\n",
    "#             break\n",
    "\n",
    "#         if word_count >= max_len:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def generate_report(\n",
    "#     fused_feat,\n",
    "#     location,\n",
    "#     max_len=MAX_REPORT_LEN,\n",
    "#     min_len=20\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     fused_feat: (1, HIDDEN_DIM)\n",
    "#     location: str\n",
    "#     \"\"\"\n",
    "\n",
    "#     decoder.eval()\n",
    "\n",
    "#     # ---- Location-specific tokens ----\n",
    "#     bos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"bos\"]\n",
    "#     )\n",
    "#     eos_id = tokenizer.convert_tokens_to_ids(\n",
    "#         LOCATION_TOKENS[location][\"eos\"]\n",
    "#     )\n",
    "\n",
    "#     generated_ids = [bos_id]\n",
    "#     word_count = 0   # counts tokens EXCLUDING BOS\n",
    "\n",
    "#     # safety cap (same as your original)\n",
    "#     for _ in range(max_len * 2):\n",
    "\n",
    "#         input_ids = torch.tensor(\n",
    "#             generated_ids,\n",
    "#             dtype=torch.long,\n",
    "#             device=DEVICE\n",
    "#         ).unsqueeze(0)   # (1, T)\n",
    "\n",
    "#         # ---- GRU decoder forward ----\n",
    "#         logits = decoder(\n",
    "#             fused_feat,\n",
    "#             input_ids\n",
    "#         )  # (1, T, vocab)\n",
    "\n",
    "#         logits = logits[0, -1] / temperature\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#         # # ==================================================\n",
    "#         # # ðŸ”‘ TOP-P (NUCLEUS) SAMPLING (UNCHANGED)\n",
    "#         # # ==================================================\n",
    "#         # sorted_probs, sorted_indices = torch.sort(\n",
    "#         #     probs, descending=True\n",
    "#         # )\n",
    "#         # cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "\n",
    "#         # cutoff = cumulative_probs > top_p\n",
    "#         # cutoff[1:] = cutoff[:-1].clone()\n",
    "#         # cutoff[0] = False\n",
    "\n",
    "#         # sorted_probs[cutoff] = 0.0\n",
    "#         # sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "#         next_id = torch.argmax(probs).item()\n",
    "\n",
    "#         # next_id = sorted_indices[next_id].item()\n",
    "\n",
    "#         # ---- Ignore early EOS ----\n",
    "#         if next_id == eos_id and word_count < min_len:\n",
    "#             continue\n",
    "\n",
    "#         generated_ids.append(next_id)\n",
    "#         word_count += 1\n",
    "\n",
    "#         # ---- Stop conditions ----\n",
    "#         if next_id == eos_iC:\n",
    "#             break\n",
    "\n",
    "#         if word_count >= max_len:\n",
    "#             break\n",
    "\n",
    "#     return tokenizer.decode(\n",
    "#         generated_ids,\n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_report(\n",
    "    fused_feat,\n",
    "    location,\n",
    "    max_len=MAX_REPORT_LEN,\n",
    "    min_len=20\n",
    "):\n",
    "    decoder.eval()\n",
    "\n",
    "    bos_id = tokenizer.convert_tokens_to_ids(\n",
    "        LOCATION_TOKENS[location][\"bos\"]\n",
    "    )\n",
    "    eos_id = tokenizer.convert_tokens_to_ids(\n",
    "        LOCATION_TOKENS[location][\"eos\"]\n",
    "    )\n",
    "\n",
    "    generated_ids = [bos_id]\n",
    "    word_count = 0\n",
    "\n",
    "    for _ in range(max_len * 2):\n",
    "\n",
    "        input_ids = torch.tensor(\n",
    "            generated_ids,\n",
    "            dtype=torch.long,\n",
    "            device=DEVICE\n",
    "        ).unsqueeze(0)   # (1, T)\n",
    "\n",
    "        logits = decoder(\n",
    "            fused_feat,\n",
    "            input_ids\n",
    "        )  # (1, T, vocab)\n",
    "\n",
    "        next_id = torch.argmax(logits[0, -1]).item()\n",
    "\n",
    "        if next_id == eos_id and word_count < min_len:\n",
    "            continue\n",
    "\n",
    "        generated_ids.append(next_id)\n",
    "        word_count += 1\n",
    "\n",
    "        if next_id == eos_id or word_count >= max_len:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04ff3156-f2d5-4174-9355-c599eb2349e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def run_inference(test_loader):\n",
    "#     ct_encoder.eval()\n",
    "#     mri_encoder.eval()\n",
    "#     text_encoder.eval()\n",
    "#     gcn.eval()\n",
    "#     fusion.eval()\n",
    "#     decoder.eval()\n",
    "\n",
    "#     generated_reports = []\n",
    "#     ground_truth_reports = []\n",
    "#     locations_all = []\n",
    "\n",
    "#     pbar = tqdm(\n",
    "#         test_loader,\n",
    "#         desc=\"Generating reports\",\n",
    "#         total=len(test_loader),\n",
    "#         leave=True\n",
    "#     )\n",
    "\n",
    "#     for batch in pbar:\n",
    "#         # ---- Move tensors ----\n",
    "#         ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "#         ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "#         mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "#         mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "#         text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "#         text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         report_ids = batch[\"report_input_ids\"]   # keep on CPU for decoding GT\n",
    "#         locations = batch[\"locations\"]\n",
    "\n",
    "#         # ---- Encode images ----\n",
    "#         ct_feats = ct_encoder(ct_imgs)\n",
    "#         mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "#         ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "#         mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "#         # ---- Encode text ----\n",
    "#         text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "#         # ---- KG embeddings ----\n",
    "#         kg_feat = get_kg_embeddings(\n",
    "#             locations, gcn, X_nodes, A_hat_dict\n",
    "#         )\n",
    "\n",
    "#         # ---- Fusion ----\n",
    "#         fused_feats = fusion(\n",
    "#             ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "#         )   # (B, EMBED_DIM)\n",
    "\n",
    "#         # ---- Generate per sample ----\n",
    "#         B = fused_feats.size(0)\n",
    "\n",
    "#         for i in range(B):\n",
    "#             gen_report = generate_report(\n",
    "#                 fused_feats[i].unsqueeze(0),\n",
    "#                 locations[i]\n",
    "#             )\n",
    "\n",
    "#             gt_report = tokenizer.decode(\n",
    "#                 report_ids[i],\n",
    "#                 skip_special_tokens=True\n",
    "#             )\n",
    "\n",
    "#             generated_reports.append(gen_report)\n",
    "#             ground_truth_reports.append(gt_report)\n",
    "#             locations_all.append(locations[i])\n",
    "\n",
    "#     return generated_reports, ground_truth_reports, locations_all\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def run_inference(test_loader):\n",
    "#     ct_encoder.eval()\n",
    "#     mri_encoder.eval()\n",
    "#     text_encoder.eval()\n",
    "#     gcn.eval()\n",
    "#     fusion.eval()\n",
    "#     decoder.eval()\n",
    "#     prefix_proj.eval()   # ðŸ”‘ FIX 1\n",
    "\n",
    "#     generated_reports = []\n",
    "#     ground_truth_reports = []\n",
    "#     locations_all = []\n",
    "\n",
    "#     pbar = tqdm(\n",
    "#         test_loader,\n",
    "#         desc=\"Generating reports\",\n",
    "#         total=len(test_loader),\n",
    "#         leave=True\n",
    "#     )\n",
    "\n",
    "#     for batch in pbar:\n",
    "#         # ---- Move tensors ----\n",
    "#         ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "#         ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "#         mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "#         mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "#         text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "#         text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "#         report_ids = batch[\"report_input_ids\"]  # CPU OK\n",
    "#         locations = batch[\"locations\"]\n",
    "\n",
    "#         # ---- Encode images ----\n",
    "#         ct_feats = ct_encoder(ct_imgs)\n",
    "#         mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "#         ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "#         mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "#         # ---- Encode text ----\n",
    "#         text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "#         # ---- KG embeddings ----\n",
    "#         kg_feat = get_kg_embeddings(\n",
    "#             locations, gcn, X_nodes, A_hat_dict\n",
    "#         )\n",
    "\n",
    "#         # ---- Fusion ----\n",
    "#         fused_feats = fusion(\n",
    "#             ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "#         )   # (B, EMBED_DIM)\n",
    "\n",
    "#         # ---- Generate per sample ----\n",
    "#         for i in range(fused_feats.size(0)):\n",
    "\n",
    "#             gen_report = generate_report(\n",
    "#                 fused_feats[i].unsqueeze(0),\n",
    "#                 locations[i]\n",
    "#             )\n",
    "\n",
    "#             # ðŸ”‘ FIX 2: align GT with training format\n",
    "#             bos_token = LOCATION_TOKENS[locations[i]][\"bos\"]\n",
    "#             gt_report = bos_token + \" \" + tokenizer.decode(\n",
    "#                 report_ids[i],\n",
    "#                 skip_special_tokens=True\n",
    "#             )\n",
    "\n",
    "#             generated_reports.append(gen_report)\n",
    "#             ground_truth_reports.append(gt_report)\n",
    "#             locations_all.append(locations[i])\n",
    "\n",
    "#     return generated_reports, ground_truth_reports, locations_all\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(test_loader):\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "    fusion.eval()\n",
    "    decoder.eval()   # âœ… GRU decoder only\n",
    "\n",
    "    generated_reports = []\n",
    "    ground_truth_reports = []\n",
    "    locations_all = []\n",
    "\n",
    "    pbar = tqdm(\n",
    "        test_loader,\n",
    "        desc=\"Generating reports\",\n",
    "        total=len(test_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"]   # CPU OK\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feats = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )   # (B, HIDDEN_DIM)\n",
    "\n",
    "        # =========================\n",
    "        # Generate per sample\n",
    "        # =========================\n",
    "        for i in range(fused_feats.size(0)):\n",
    "\n",
    "            gen_report = generate_report(\n",
    "                fused_feats[i].unsqueeze(0),\n",
    "                locations[i]\n",
    "            )\n",
    "\n",
    "            # ==================================================\n",
    "            # ðŸ”‘ Align GT format with training (KEEP THIS)\n",
    "            # ==================================================\n",
    "            bos_token = LOCATION_TOKENS[locations[i]][\"bos\"]\n",
    "            gt_report = bos_token + \" \" + tokenizer.decode(\n",
    "                report_ids[i],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            generated_reports.append(gen_report)\n",
    "            ground_truth_reports.append(gt_report)\n",
    "            locations_all.append(locations[i])\n",
    "\n",
    "    return generated_reports, ground_truth_reports, locations_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05f64d93-576a-469e-afb2-514c63f9c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2a24711-5da5-482e-bceb-d6b1fa89ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_decoder_only_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_decoder_only_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8afed621-58d0-44aa-8620-5df2c6253752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STAGE 2: Unfreeze Fusion\n",
    "# =========================\n",
    "\n",
    "# Keep encoders frozen\n",
    "freeze_module(ct_encoder)\n",
    "freeze_module(mri_encoder)\n",
    "freeze_module(text_encoder)\n",
    "freeze_module(gcn)\n",
    "\n",
    "# Unfreeze fusion\n",
    "for p in fusion.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Decoder already trainable\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d512f5a8-bea7-44a0-a587-5d72288256c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trainable Parameters Check ===\n",
      "CT Encoder: 0 trainable params\n",
      "MRI Encoder: 0 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 0 trainable params\n",
      "Fusion: 524,800 trainable params\n",
      "Decoder: 25,056,837 trainable params\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Trainable Parameters Check ===\")\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f03aa75-e2ad-4ab5-8def-19d217b6357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-2 optimizer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Optimizer for Stage 2\n",
    "# =========================\n",
    "\n",
    "stage2_params = []\n",
    "\n",
    "stage2_params += [p for p in fusion.parameters() if p.requires_grad]\n",
    "stage2_params += [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    stage2_params,\n",
    "    lr=1e-4,          # ðŸ”‘ LOWER LR\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "print(\"Stage-2 optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "228ca87f-834d-4984-ae4e-1f0bf1f7c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_stage2(train_loader):\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    gcn.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Stage-2 Training (Fusion + Decoder)\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        with torch.no_grad():\n",
    "            ct_feats = ct_encoder(ct_imgs)\n",
    "            mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "            ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "            mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "            text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "            kg_feat = get_kg_embeddings(\n",
    "                locations, gcn, X_nodes, A_hat_dict\n",
    "            )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # Location-specific BOS\n",
    "        # =========================\n",
    "        B = report_ids.size(0)\n",
    "\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        decoder_inputs = report_ids[:, :-1]\n",
    "        targets = report_ids[:, 1:]\n",
    "\n",
    "        logits = decoder(fused_feat, decoder_inputs)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b8adbdd-ed1a-49f4-ae84-8fb3175aae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.77it/s, loss=3.7537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 1/10 | Loss: 3.8506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.88it/s, loss=4.0856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 2/10 | Loss: 3.7810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.83it/s, loss=3.7919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 3/10 | Loss: 3.7277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.79it/s, loss=3.0253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 4/10 | Loss: 3.6623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.88it/s, loss=3.3591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 5/10 | Loss: 3.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.87it/s, loss=3.7891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 6/10 | Loss: 3.5618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.83it/s, loss=3.4704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 7/10 | Loss: 3.5228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.80it/s, loss=2.8754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 8/10 | Loss: 3.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.90it/s, loss=3.4523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 9/10 | Loss: 3.4739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-2 Training (Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:40<00:00,  3.97it/s, loss=3.5467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 10/10 | Loss: 3.4526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STAGE2_EPOCHS = 10\n",
    "\n",
    "for epoch in range(STAGE2_EPOCHS):\n",
    "    loss = train_one_epoch_stage2(train_loader)\n",
    "    print(\n",
    "        f\"[Stage-2] Epoch {epoch+1}/{STAGE2_EPOCHS} | \"\n",
    "        f\"Loss: {loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc91865a-c00e-4d9e-8b09-58c2ba96ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6488cdbd-fecf-47a3-8daa-aae74026195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_fusion_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_fusion_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37546f23-59c4-4f12-bcd5-52a371befa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STAGE 3: Unfreeze GCN\n",
    "# =========================\n",
    "\n",
    "# Keep encoders frozen\n",
    "freeze_module(ct_encoder)\n",
    "freeze_module(mri_encoder)\n",
    "freeze_module(text_encoder)\n",
    "\n",
    "# Unfreeze GCN\n",
    "for p in gcn.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Fusion + Decoder remain trainable\n",
    "for p in fusion.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b702d08-add6-4b2e-8fbb-a86a7e6d9a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage-3 Trainable Params Check ===\n",
      "CT Encoder: 0 trainable params\n",
      "MRI Encoder: 0 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 1,192,448 trainable params\n",
      "Fusion: 524,800 trainable params\n",
      "Decoder: 25,056,837 trainable params\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Stage-3 Trainable Params Check ===\")\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "604e3a75-4514-40ac-8232-c55ec514f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-3 optimizer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Optimizer for Stage 3\n",
    "# =========================\n",
    "\n",
    "stage3_params = []\n",
    "\n",
    "stage3_params += [p for p in gcn.parameters() if p.requires_grad]\n",
    "stage3_params += [p for p in fusion.parameters() if p.requires_grad]\n",
    "stage3_params += [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    stage3_params,\n",
    "    lr=5e-5,          # ðŸ”‘ LOWER LR (KG is sensitive)\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "print(\"Stage-3 optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c8d8db3-7df9-40a5-a84a-a516bd89e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_stage3(train_loader):\n",
    "    gcn.train()\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "\n",
    "    ct_encoder.eval()\n",
    "    mri_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Stage-3 Training (GCN + Fusion + Decoder)\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode frozen modalities\n",
    "        # =========================\n",
    "        with torch.no_grad():\n",
    "            ct_feats = ct_encoder(ct_imgs)\n",
    "            mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "            ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "            mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "            text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        # =========================\n",
    "        # GCN now TRAINABLE\n",
    "        # =========================\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # Location BOS + decoding\n",
    "        # =========================\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        decoder_inputs = report_ids[:, :-1]\n",
    "        targets = report_ids[:, 1:]\n",
    "\n",
    "        logits = decoder(fused_feat, decoder_inputs)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4308dc2b-d201-4b13-a79e-58a1ee231890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.76it/s, loss=3.3338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 1/10 | Loss: 3.4655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.69it/s, loss=3.3469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 2/10 | Loss: 3.4316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.73it/s, loss=4.5418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 3/10 | Loss: 3.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.72it/s, loss=3.2825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 4/10 | Loss: 3.3709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.65it/s, loss=3.5814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 5/10 | Loss: 3.3565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.83it/s, loss=4.0656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 6/10 | Loss: 3.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:41<00:00,  3.86it/s, loss=4.0433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 7/10 | Loss: 3.3168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:52<00:00,  3.04it/s, loss=2.7069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 8/10 | Loss: 3.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:47<00:00,  3.40it/s, loss=3.1947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 9/10 | Loss: 3.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-3 Training (GCN + Fusion + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:42<00:00,  3.77it/s, loss=3.4264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-3] Epoch 10/10 | Loss: 3.2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STAGE3_EPOCHS = 10\n",
    "\n",
    "for epoch in range(STAGE3_EPOCHS):\n",
    "    loss = train_one_epoch_stage3(train_loader)\n",
    "    print(\n",
    "        f\"[Stage-3] Epoch {epoch+1}/{STAGE3_EPOCHS} | \"\n",
    "        f\"Loss: {loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42d9dd06-2eb2-4803-831f-f920c0f7cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aee280f3-2332-4c06-a944-4132a2cfc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_GCN_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_GCN_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "632301f9-74de-4e11-ac35-7d5cdf181778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helper: Unfreeze ResNet layer4 only\n",
    "# =========================\n",
    "\n",
    "def unfreeze_resnet_layer4(resnet_model):\n",
    "    # Freeze everything\n",
    "    for p in resnet_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Unfreeze layer4\n",
    "    for p in resnet_model.layer4.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Keep FC trainable (already replaced)\n",
    "    for p in resnet_model.fc.parameters():\n",
    "        p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15bd0d09-2e95-4777-a417-922177c2237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STAGE 4: Partial Image Unfreeze\n",
    "# =========================\n",
    "\n",
    "unfreeze_resnet_layer4(ct_encoder.cnn)\n",
    "unfreeze_resnet_layer4(mri_encoder.cnn)\n",
    "\n",
    "# Text encoder stays frozen\n",
    "freeze_module(text_encoder)\n",
    "\n",
    "# GCN + Fusion + Decoder remain trainable\n",
    "for p in gcn.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in fusion.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28b2b9a6-37c7-440b-8d8e-03cf223cd20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage-4 Trainable Params Check ===\n",
      "CT Encoder: 8,525,056 trainable params\n",
      "MRI Encoder: 8,525,056 trainable params\n",
      "Text Encoder: 0 trainable params\n",
      "GCN: 1,192,448 trainable params\n",
      "Fusion: 524,800 trainable params\n",
      "Decoder: 25,056,837 trainable params\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Stage-4 Trainable Params Check ===\")\n",
    "count_trainable(\"CT Encoder\", ct_encoder)\n",
    "count_trainable(\"MRI Encoder\", mri_encoder)\n",
    "count_trainable(\"Text Encoder\", text_encoder)\n",
    "count_trainable(\"GCN\", gcn)\n",
    "count_trainable(\"Fusion\", fusion)\n",
    "count_trainable(\"Decoder\", decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29f42782-218d-43f4-856c-2bffcef86e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage-4 optimizer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Optimizer for Stage 4\n",
    "# =========================\n",
    "\n",
    "stage4_params = []\n",
    "\n",
    "stage4_params += [p for p in ct_encoder.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in mri_encoder.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in gcn.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in fusion.parameters() if p.requires_grad]\n",
    "stage4_params += [p for p in decoder.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    stage4_params,\n",
    "    lr=1e-5,          # ðŸ”‘ VERY IMPORTANT: small LR\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=5,\n",
    "    gamma=0.5\n",
    ")\n",
    "\n",
    "print(\"Stage-4 optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be5f7d04-4d8a-4331-98fe-dc33707ec07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_stage4(train_loader):\n",
    "    ct_encoder.train()\n",
    "    mri_encoder.train()\n",
    "    gcn.train()\n",
    "    fusion.train()\n",
    "    decoder.train()\n",
    "\n",
    "    text_encoder.eval()   # still frozen\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=\"Stage-4 Training (Visual + KG + Decoder)\",\n",
    "        total=len(train_loader),\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # =========================\n",
    "        # Move tensors\n",
    "        # =========================\n",
    "        ct_imgs = batch[\"ct_images\"].to(DEVICE)\n",
    "        ct_masks = batch[\"ct_masks\"].to(DEVICE)\n",
    "\n",
    "        mri_imgs = batch[\"mri_images\"].to(DEVICE)\n",
    "        mri_masks = batch[\"mri_masks\"].to(DEVICE)\n",
    "\n",
    "        text_ids = batch[\"text_input_ids\"].to(DEVICE)\n",
    "        text_mask = batch[\"text_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        report_ids = batch[\"report_input_ids\"].to(DEVICE)\n",
    "        locations = batch[\"locations\"]\n",
    "\n",
    "        # =========================\n",
    "        # Encode modalities\n",
    "        # =========================\n",
    "        ct_feats = ct_encoder(ct_imgs)\n",
    "        mri_feats = mri_encoder(mri_imgs)\n",
    "\n",
    "        ct_pooled = masked_mean_pooling(ct_feats, ct_masks)\n",
    "        mri_pooled = masked_mean_pooling(mri_feats, mri_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_feat = text_encoder(text_ids, text_mask)\n",
    "\n",
    "        kg_feat = get_kg_embeddings(\n",
    "            locations, gcn, X_nodes, A_hat_dict\n",
    "        )\n",
    "\n",
    "        fused_feat = fusion(\n",
    "            ct_pooled, mri_pooled, text_feat, kg_feat\n",
    "        )\n",
    "\n",
    "        # =========================\n",
    "        # Location BOS + decoding\n",
    "        # =========================\n",
    "        bos_ids = torch.tensor(\n",
    "            [\n",
    "                tokenizer.convert_tokens_to_ids(\n",
    "                    LOCATION_TOKENS[loc][\"bos\"]\n",
    "                )\n",
    "                for loc in locations\n",
    "            ],\n",
    "            device=report_ids.device\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        report_ids = torch.cat([bos_ids, report_ids], dim=1)\n",
    "\n",
    "        decoder_inputs = report_ids[:, :-1]\n",
    "        targets = report_ids[:, 1:]\n",
    "\n",
    "        logits = decoder(fused_feat, decoder_inputs)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1a4dd6f-6b12-4065-8617-b7b9f7d59502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.65it/s, loss=3.4281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 1/6 | Loss: 3.2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:43<00:00,  3.64it/s, loss=3.0294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 2/6 | Loss: 3.2649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.62it/s, loss=2.2342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 3/6 | Loss: 3.2571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:45<00:00,  3.53it/s, loss=1.8285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 4/6 | Loss: 3.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:45<00:00,  3.49it/s, loss=2.5005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 5/6 | Loss: 3.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-4 Training (Visual + KG + Decoder): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:44<00:00,  3.59it/s, loss=3.2908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-4] Epoch 6/6 | Loss: 3.2426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "STAGE4_EPOCHS = 6\n",
    "\n",
    "for epoch in range(STAGE4_EPOCHS):\n",
    "    loss = train_one_epoch_stage4(train_loader)\n",
    "    print(\n",
    "        f\"[Stage-4] Epoch {epoch+1}/{STAGE4_EPOCHS} | \"\n",
    "        f\"Loss: {loss:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b556835-ff68-40ec-9da5-2d052abf33a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reports: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "generated_reports, ground_truth_reports, locations_all = run_inference(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f60cb493-457a-4144-9428-1a1b21a6c6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_encoder_unfrozen.csv\n",
      "Total samples: 34\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"location\": locations_all,\n",
    "    \"generated_report\": generated_reports,\n",
    "    \"ground_truth_report\": ground_truth_reports\n",
    "})\n",
    "\n",
    "save_path = r\"C:\\fyp_manish_shyam_phase2\\results\\generated_vs_gt_reports_encoder_unfrozen.csv\"\n",
    "results_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Saved results to: {save_path}\")\n",
    "print(\"Total samples:\", len(results_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d433c8e-655c-4c1e-8312-62aa0c13a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHECKPOINT_DIR = r\"C:\\fyp_manish_shyam_phase2\\checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77a957b4-52c6-4b02-8622-82fa51c00657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    epoch,\n",
    "    stage,\n",
    "    ct_encoder,\n",
    "    mri_encoder,\n",
    "    text_encoder,\n",
    "    gcn,\n",
    "    fusion,\n",
    "    decoder,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tokenizer,\n",
    "    path\n",
    "):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"stage\": stage,\n",
    "\n",
    "        \"ct_encoder\": ct_encoder.state_dict(),\n",
    "        \"mri_encoder\": mri_encoder.state_dict(),\n",
    "        \"text_encoder\": text_encoder.state_dict(),\n",
    "        \"gcn\": gcn.state_dict(),\n",
    "        \"fusion\": fusion.state_dict(),\n",
    "        \"decoder\": decoder.state_dict(),\n",
    "\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "\n",
    "        \"tokenizer\": tokenizer\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"âœ… Checkpoint saved to: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2121edd-5ed3-487d-9131-97dccc81eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Checkpoint saved to: C:\\fyp_manish_shyam_phase2\\checkpoints\\model_stage_final.pt\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(\n",
    "    epoch=epoch,\n",
    "    stage=\"stage_final\",\n",
    "    ct_encoder=ct_encoder,\n",
    "    mri_encoder=mri_encoder,\n",
    "    text_encoder=text_encoder,\n",
    "    gcn=gcn,\n",
    "    fusion=fusion,\n",
    "    decoder=decoder,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    tokenizer=tokenizer,\n",
    "    path=os.path.join(CHECKPOINT_DIR, \"model_stage_final.pt\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
